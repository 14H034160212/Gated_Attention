{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANet_Trec_FF_Aux.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twisha96/Gated_Attention/blob/master/GANet_Trec_FF_Aux.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D0T3nxESJstz",
        "outputId": "6892dbd9-24b3-4a0c-93f7-6d82b4cb3fcb",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!pip3 install torchtext==0.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.4 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4) (1.18.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.4) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a9sHZi8rIXBC",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions\n",
        "import torch.optim as optim\n",
        "from torch import nn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "\n",
        "# from models.LSTM import LSTMClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6CtSKjSRaQdd",
        "colab": {}
      },
      "source": [
        "# model.py\n",
        "\n",
        "class GANet(torch.nn.Module):\n",
        "    def __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, aux_hidden_size = 100, backbone_hidden_size = 100, tau = 1, biDirectional_aux = False, biDirectional_backbone = False):\n",
        "        super(GANet, self).__init__() \n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "        output_size : 6 = (For TREC dataset)\n",
        "        hidden_sie : Size of the hidden_state of the LSTM   (// Later BiLSTM)\n",
        "        vocab_size : Size of the vocabulary containing unique words\n",
        "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
        "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\n",
        "        --------\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_length = embedding_length\n",
        "        self.aux_hidden_size = aux_hidden_size\n",
        "        self.backbone_hidden_size = backbone_hidden_size \n",
        "        self.mlp_out_size = mlp_out_size\n",
        "        self.biDirectional_aux = biDirectional_aux\n",
        "        self.biDirectional_backbone = biDirectional_backbone\n",
        "        self.tau = tau\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
        "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
        "\n",
        "        self.auxiliary = AuxiliaryNet(self.batch_size, self.aux_hidden_size, self.embedding_length, self.biDirectional_aux, tau = self.tau)\n",
        "        self.backbone = BackboneNet(self.batch_size, self.backbone_hidden_size, self.embedding_length, self.biDirectional_backbone)\n",
        "\n",
        "        if(self.biDirectional_backbone):\n",
        "            self.mlp = MLP(self.backbone_hidden_size * 2, self.mlp_out_size)\n",
        "            self.FF = nn.Linear(self.backbone_hidden_size * 2,num_classes)\n",
        "        else:\n",
        "            self.mlp = MLP(self.backbone_hidden_size, self.mlp_out_size)\n",
        "            self.FF = nn.Linear(self.backbone_hidden_size,num_classes)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def masked_Softmax(self, logits, mask):\n",
        "        mask_bool = mask>0\n",
        "        logits[~mask_bool] = float('-inf')\n",
        "        return torch.softmax(logits, dim=1)\n",
        "\n",
        "    \n",
        "    def forward(self,input_sequence, is_train = True):\n",
        "        input_ = self.word_embeddings(input_sequence)\n",
        "        g_t, p_t = self.auxiliary(input_, is_train)\n",
        "        out_lstm = self.backbone(input_)\n",
        "\n",
        "        if is_train:\n",
        "            e_t = self.mlp(out_lstm)\n",
        "#             alpha = torch.softmax(e_t*g_t, dim = 1)\n",
        "#             pdb.set_trace()\n",
        "            alpha_numerator = torch.exp(e_t)*g_t\n",
        "            alpha_denomenator = torch.sum(alpha_numerator, dim=1).repeat(e_t.shape[1],1,1).transpose(0,1)\n",
        "            alpha = alpha_numerator/alpha_denomenator\n",
        "        else:\n",
        "            e_t = self.mlp(out_lstm)               # change if possible!\n",
        "            alpha = self.masked_Softmax(e_t, g_t)\n",
        "            \n",
        "        c_t = torch.bmm(alpha.transpose(1,2), out_lstm)\n",
        "        logits = self.FF(c_t)\n",
        "        logits = self.tanh(logits)\n",
        "        final_output = torch.softmax(logits, dim = -1)\n",
        "        # final_output = final_output.max(2)[1]\n",
        "        final_output = final_output.squeeze(1)\n",
        "\n",
        "        return final_output, g_t, alpha, p_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pdl-jRmHb5KA",
        "colab": {}
      },
      "source": [
        "class AuxiliaryNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "    aux_hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
        "    embedding_length : Embeddding dimension of GloVe word embeddings\n",
        "    --------\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, auxiliary_hidden_size, embedding_length, biDirectional = False, num_layers = 1, tau=1):\n",
        "        super(AuxiliaryNet, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = auxiliary_hidden_size\n",
        "        self.embedding_length = embedding_length\n",
        "        self.biDirectional = biDirectional\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.mlp = Aux_MLP(self.embedding_length)\n",
        "\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.tau = tau\n",
        "\n",
        "\n",
        "    def forward(self, input_sequence, is_train = True, batch_size=None):\n",
        "\n",
        "        # input : Dimensions (batch_size x seq_len x embedding_length)\n",
        "        # out_lstm, (final_hidden_state, final_cell_state) = self.aux_lstm(input_sequence) # ouput dim: (batch_size x seq_len x hidden_size) \n",
        "        \n",
        "        out_mlp = self.mlp(input_sequence)                                           # p_t dim: (batch_size x seq_len x 1)\n",
        "        p_t = self.sigmoid(out_mlp)\n",
        "\n",
        "        if is_train:\n",
        "            p_t = p_t.repeat(1,1,2)\n",
        "            p_t[:,:,0] = 1 - p_t[:,:,0] \n",
        "            g_hat = F.gumbel_softmax(p_t, self.tau, hard=False)\n",
        "#             print(\"p_t: \", p_t)\n",
        "            g_t = g_hat[:,:,1]\n",
        "            g_t = g_t.reshape(g_t.shape[0], g_t.shape[1], 1)\n",
        "#             print(\"g_t Gumbel: \", g_t)\n",
        "        else:\n",
        "            # size : same as p_t [ batch_size x seq_len x 1]\n",
        "#             print(\"Underlying probability distribution: \", p_t)\n",
        "            m = torch.distributions.bernoulli.Bernoulli(p_t)   \n",
        "#             pdb.set_trace()\n",
        "            g_t = m.sample()\n",
        "            \n",
        "            # We do not want all the values of g_t for a sample to be 0\n",
        "            # Make all g_t's 1 when all are zero\n",
        "            gt_sum = g_t.sum(1)\n",
        "            x = (gt_sum == 0).nonzero()\n",
        "            x = x[:,0]\n",
        "            for i in x:\n",
        "                g_t[i,:,:] = torch.ones(g_t[i,:,:].shape)\n",
        "#             print(\"All zero: \", len(x))\n",
        "        return g_t, p_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YT9xWP_MAVmq",
        "colab": {}
      },
      "source": [
        "class BackboneNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "        backbone_hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
        "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
        "        --------\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, backbone_hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
        "\n",
        "        super(BackboneNet, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = backbone_hidden_size\n",
        "        self.embedding_length = embedding_length\n",
        "        self.biDirectional\t= biDirectional\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.backbone_lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
        "\n",
        "    def forward(self, input_sequence, batch_size=None):\n",
        "        out_lstm, (final_hidden_state, final_cell_state) = self.backbone_lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
        "        return out_lstm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yK5aAf8nVus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Aux_MLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, layer1_dim= 64, layer2_dim = 32 ):\n",
        "        super(Aux_MLP, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim   # Embedding dimension\n",
        "        self.layer1_dim = layer1_dim\n",
        "        self.layer2_dim = layer2_dim\n",
        "\n",
        "        # self.output_dim = output_dim  # \n",
        "\n",
        "        self.ff_1 = nn.Linear(self.input_dim, self.layer1_dim)\n",
        "#         self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.ff_2 =  nn.Linear(self.layer1_dim,self.layer2_dim)\n",
        "        self.ff_3 = nn.Linear(self.layer2_dim,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        out_1 = self.ff_1(x)\n",
        "#         out_relu = self.relu(out_1)\n",
        "#         out_2 = self.ff_2(out_relu)\n",
        "        out_tanh_1 = self.tanh(out_1)\n",
        "        out_2 = self.ff_2(out_tanh_1)\n",
        "        out_tanh_2 = self.tanh(out_2)\n",
        "        out_3 = self.ff_3(out_tanh_2)\n",
        "#         out_sigmoid = self.sigmoid(out_2)\n",
        "\n",
        "        return out_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x_eU0HR-j8lW",
        "colab": {}
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
        "#         self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.ff_2 = nn.Linear(self.output_dim,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        out_1 = self.ff_1(x)\n",
        "#         out_relu = self.relu(out_1)\n",
        "#         out_2 = self.ff_2(out_relu)\n",
        "        out_tanh = self.tanh(out_1)\n",
        "        out_2 = self.ff_2(out_tanh)\n",
        "#         out_sigmoid = self.sigmoid(out_2)\n",
        "\n",
        "        return out_2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Swz_WT2mS3zq",
        "colab": {}
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "    \n",
        "def train_model(model, optim, train_iter, epoch, batch_size):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    # model.cuda()\n",
        "#     optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch.text[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        if (text.size()[0] is not batch_size):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction, g_t, alpha, p_t = model(text, is_train = True)\n",
        "        # print(\"prediction = \", prediction.shape)\n",
        "        # print(\"target = \", target.shape)\n",
        "        # print(\"prediction = \", prediction)\n",
        "        # print(\"target = \", target)\n",
        "        \n",
        "        # Modifies loss function\n",
        "        loss = loss_fn(prediction, target, g_t)\n",
        "\n",
        "        # Defualt - Cross entropy loss funtion\n",
        "#         loss = loss_fn(prediction, target)\n",
        "        \n",
        "        # print(\"loss = \", loss)\n",
        "        \n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        # if steps % 10 == 0:\n",
        "            # print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "            # break\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    total_attention =  0\n",
        "    total_samples = 0 \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch.text[0]\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction, g_t, alpha, p_t = model(text, is_train = False)\n",
        "            # Sanity check\n",
        "            # print(\"Test Prediction: \", prediction)\n",
        "            # print(\"Gate values: \", g_t)\n",
        "\n",
        "            # For density calculation\n",
        "            total_attention += torch.sum(g_t)\n",
        "            # print(total_attention)\n",
        "            # print(g_t.shape)\n",
        "            total_samples += g_t.shape[0] * g_t.shape[1]\n",
        "            \n",
        "            # Modifies loss function\n",
        "            loss = loss_fn(prediction, target, g_t)\n",
        "\n",
        "            # Defualt - Cross entropy loss funtion\n",
        "#             loss =  loss_fn(prediction, target)\n",
        "            \n",
        "            if math.isnan(loss.item()):\n",
        "                print(prediction, target)\n",
        "            \n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "            \n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter), total_attention/total_samples "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vAjlrmkGSEQQ",
        "colab": {}
      },
      "source": [
        "# data.py\n",
        "def load_TREC_data(batch_size= 16, embedding_length = 100):\n",
        "    # set up fields\n",
        "    tokenize = lambda x: x.split()\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length= 10)\n",
        "    # LABEL = data.LabelField()\n",
        "    LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "    # make splits for data\n",
        "    train, test = datasets.TREC.splits(TEXT, LABEL)\n",
        "    train, valid = train.split() \n",
        "    \n",
        "    # build the vocabulary\n",
        "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=embedding_length))\n",
        "    LABEL.build_vocab(train)\n",
        "    print(LABEL.vocab.__dict__)\n",
        "\n",
        "    # make iterator for splits\n",
        "    train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
        "      (train, valid, test), batch_size= batch_size, device=0)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "\n",
        "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xHuvlEncW_tw",
        "outputId": "dbe599ae-6737-4aab-c7f4-0927fd228352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "# main.py\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_TREC_data()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading train_5500.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 4.30MB/s]\n",
            "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 1.17MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading TREC_10.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                          \n",
            "100%|█████████▉| 399218/400000 [00:18<00:00, 21566.37it/s]WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'freqs': Counter({'ENTY': 871, 'HUM': 835, 'DESC': 832, 'NUM': 624, 'LOC': 598, 'ABBR': 56}), 'itos': ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR'], 'unk_index': None, 'stoi': defaultdict(None, {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5}), 'vectors': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dkoJVXJ2mzCD",
        "colab": {}
      },
      "source": [
        "def loss_fn(output, target, g_t, lambda_ = 1e-4):\n",
        "    T = g_t.shape[0]*g_t.shape[1]\n",
        "    # loss = -nn.LogSoftmax(output[target], dim = 1) + (lambda_ * torch.sum(g_t))/T\n",
        "    loss = F.cross_entropy(output, target) + (lambda_ * (torch.sum(g_t)/T))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PImJSOJmzQgT",
        "colab": {}
      },
      "source": [
        "# Over-writing the loss function to simple cross entropy loss\n",
        "# loss_fn = F.cross_entropy\n",
        "\n",
        "learning_rate = 2e-5\n",
        "batch_size = 16\n",
        "output_size = 6\n",
        "hidden_size = 256\n",
        "embedding_length = 100\n",
        "num_classes = 6\n",
        "mlp_out_size = 32\n",
        "weights = word_embeddings\n",
        "aux_hidden_size = 100\n",
        "batch_hidden_size = 100\n",
        "tau = 1\n",
        "\n",
        "model = GANet(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, tau= tau, biDirectional_aux=True, biDirectional_backbone=True)\n",
        "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EcU6SSW8bDln",
        "outputId": "f6ffa4b6-9437-4953-fab7-8a0c723342e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_bad_epochs = 0\n",
        "epoch = 0\n",
        "least_loss = float('inf')\n",
        "training_stats = pd.DataFrame(columns=['Epoch', 'Train_Loss', 'Train_Acc', 'Val_Loss', 'Val_Acc', 'Val_Density'])\n",
        "\n",
        "while(True):\n",
        "    train_loss, train_acc = train_model(model, optim, train_iter, epoch, batch_size)\n",
        "    val_loss, val_acc, val_density = eval_model(model, valid_iter) \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    if val_loss < least_loss:\n",
        "        least_loss = val_loss\n",
        "        num_bad_epochs = 0\n",
        "        print(\"*** Least validation loss\")\n",
        "        torch.save(model.state_dict(), \"BiLSTM+BiLSTM_BS_16\")\n",
        "    else:\n",
        "        num_bad_epochs += 1\n",
        "#     print(f'Epoch: {epoch+1:2}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%') \n",
        "    print(f'Val Loss: {val_loss:3f}, Val Acc: {val_acc:.2f}%, Val Density: {val_density:.4f}')\n",
        "    print(\"-------------\")\n",
        "    \n",
        "    training_stats = training_stats.append(\n",
        "        pd.Series([epoch+1, train_loss, train_acc, val_loss, val_acc, val_density], index=training_stats.columns), \n",
        "        ignore_index=True)\n",
        "#     if num_bad_epochs >= 10:\n",
        "#         break\n",
        "        \n",
        "    epoch += 1\n",
        "    if epoch == 50:\n",
        "        break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "*** Least validation loss\n",
            "Train Loss: 1.628, Train Acc: 52.59%\n",
            "Val Loss: 1.556285, Val Acc: 65.72%, Val Density: 0.4880\n",
            "-------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399218/400000 [00:30<00:00, 21566.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02\n",
            "*** Least validation loss\n",
            "Train Loss: 1.526, Train Acc: 71.10%\n",
            "Val Loss: 1.527702, Val Acc: 71.91%, Val Density: 0.4930\n",
            "-------------\n",
            "Epoch: 03\n",
            "*** Least validation loss\n",
            "Train Loss: 1.496, Train Acc: 76.41%\n",
            "Val Loss: 1.523074, Val Acc: 72.51%, Val Density: 0.4616\n",
            "-------------\n",
            "Epoch: 04\n",
            "*** Least validation loss\n",
            "Train Loss: 1.481, Train Acc: 79.34%\n",
            "Val Loss: 1.515676, Val Acc: 74.09%, Val Density: 0.4869\n",
            "-------------\n",
            "Epoch: 05\n",
            "Train Loss: 1.470, Train Acc: 81.67%\n",
            "Val Loss: 1.516554, Val Acc: 73.67%, Val Density: 0.4902\n",
            "-------------\n",
            "Epoch: 06\n",
            "*** Least validation loss\n",
            "Train Loss: 1.457, Train Acc: 84.13%\n",
            "Val Loss: 1.508507, Val Acc: 75.18%, Val Density: 0.4530\n",
            "-------------\n",
            "Epoch: 07\n",
            "*** Least validation loss\n",
            "Train Loss: 1.448, Train Acc: 86.11%\n",
            "Val Loss: 1.503547, Val Acc: 76.40%, Val Density: 0.4620\n",
            "-------------\n",
            "Epoch: 08\n",
            "*** Least validation loss\n",
            "Train Loss: 1.441, Train Acc: 87.21%\n",
            "Val Loss: 1.501201, Val Acc: 77.12%, Val Density: 0.5057\n",
            "-------------\n",
            "Epoch: 09\n",
            "*** Least validation loss\n",
            "Train Loss: 1.433, Train Acc: 88.83%\n",
            "Val Loss: 1.498095, Val Acc: 77.91%, Val Density: 0.5100\n",
            "-------------\n",
            "Epoch: 10\n",
            "Train Loss: 1.429, Train Acc: 89.67%\n",
            "Val Loss: 1.499159, Val Acc: 77.31%, Val Density: 0.5139\n",
            "-------------\n",
            "Epoch: 11\n",
            "*** Least validation loss\n",
            "Train Loss: 1.425, Train Acc: 90.25%\n",
            "Val Loss: 1.494925, Val Acc: 78.34%, Val Density: 0.5230\n",
            "-------------\n",
            "Epoch: 12\n",
            "Train Loss: 1.422, Train Acc: 91.06%\n",
            "Val Loss: 1.500166, Val Acc: 77.00%, Val Density: 0.5268\n",
            "-------------\n",
            "Epoch: 13\n",
            "Train Loss: 1.420, Train Acc: 91.32%\n",
            "Val Loss: 1.495342, Val Acc: 78.28%, Val Density: 0.5410\n",
            "-------------\n",
            "Epoch: 14\n",
            "*** Least validation loss\n",
            "Train Loss: 1.418, Train Acc: 91.63%\n",
            "Val Loss: 1.490486, Val Acc: 78.94%, Val Density: 0.5358\n",
            "-------------\n",
            "Epoch: 15\n",
            "Train Loss: 1.419, Train Acc: 91.50%\n",
            "Val Loss: 1.491834, Val Acc: 78.70%, Val Density: 0.5302\n",
            "-------------\n",
            "Epoch: 16\n",
            "*** Least validation loss\n",
            "Train Loss: 1.414, Train Acc: 92.34%\n",
            "Val Loss: 1.487510, Val Acc: 79.49%, Val Density: 0.5267\n",
            "-------------\n",
            "Epoch: 17\n",
            "Train Loss: 1.414, Train Acc: 92.34%\n",
            "Val Loss: 1.490074, Val Acc: 79.37%, Val Density: 0.5348\n",
            "-------------\n",
            "Epoch: 18\n",
            "*** Least validation loss\n",
            "Train Loss: 1.414, Train Acc: 92.42%\n",
            "Val Loss: 1.485773, Val Acc: 79.61%, Val Density: 0.5240\n",
            "-------------\n",
            "Epoch: 19\n",
            "Train Loss: 1.415, Train Acc: 92.21%\n",
            "Val Loss: 1.487042, Val Acc: 79.92%, Val Density: 0.5224\n",
            "-------------\n",
            "Epoch: 20\n",
            "Train Loss: 1.412, Train Acc: 92.81%\n",
            "Val Loss: 1.494529, Val Acc: 77.97%, Val Density: 0.5224\n",
            "-------------\n",
            "Epoch: 21\n",
            "Train Loss: 1.412, Train Acc: 92.73%\n",
            "Val Loss: 1.490775, Val Acc: 79.13%, Val Density: 0.5122\n",
            "-------------\n",
            "Epoch: 22\n",
            "Train Loss: 1.409, Train Acc: 93.25%\n",
            "Val Loss: 1.489731, Val Acc: 79.67%, Val Density: 0.4970\n",
            "-------------\n",
            "Epoch: 23\n",
            "Train Loss: 1.409, Train Acc: 93.33%\n",
            "Val Loss: 1.496557, Val Acc: 77.79%, Val Density: 0.5343\n",
            "-------------\n",
            "Epoch: 24\n",
            "Train Loss: 1.412, Train Acc: 92.81%\n",
            "Val Loss: 1.488464, Val Acc: 79.43%, Val Density: 0.4674\n",
            "-------------\n",
            "Epoch: 25\n",
            "Train Loss: 1.408, Train Acc: 93.46%\n",
            "Val Loss: 1.491631, Val Acc: 78.64%, Val Density: 0.5252\n",
            "-------------\n",
            "Epoch: 26\n",
            "Train Loss: 1.407, Train Acc: 93.70%\n",
            "Val Loss: 1.487495, Val Acc: 79.67%, Val Density: 0.5505\n",
            "-------------\n",
            "Epoch: 27\n",
            "Train Loss: 1.406, Train Acc: 93.93%\n",
            "Val Loss: 1.486421, Val Acc: 79.98%, Val Density: 0.5460\n",
            "-------------\n",
            "Epoch: 28\n",
            "Train Loss: 1.406, Train Acc: 93.96%\n",
            "Val Loss: 1.486361, Val Acc: 80.16%, Val Density: 0.5474\n",
            "-------------\n",
            "Epoch: 29\n",
            "Train Loss: 1.405, Train Acc: 93.99%\n",
            "Val Loss: 1.490585, Val Acc: 79.13%, Val Density: 0.5451\n",
            "-------------\n",
            "Epoch: 30\n",
            "Train Loss: 1.405, Train Acc: 94.14%\n",
            "Val Loss: 1.490200, Val Acc: 79.07%, Val Density: 0.5432\n",
            "-------------\n",
            "Epoch: 31\n",
            "Train Loss: 1.404, Train Acc: 94.12%\n",
            "Val Loss: 1.493039, Val Acc: 78.64%, Val Density: 0.5519\n",
            "-------------\n",
            "Epoch: 32\n",
            "Train Loss: 1.406, Train Acc: 93.88%\n",
            "Val Loss: 1.489063, Val Acc: 78.88%, Val Density: 0.5435\n",
            "-------------\n",
            "Epoch: 33\n",
            "Train Loss: 1.406, Train Acc: 93.85%\n",
            "Val Loss: 1.489563, Val Acc: 78.94%, Val Density: 0.5382\n",
            "-------------\n",
            "Epoch: 34\n",
            "Train Loss: 1.405, Train Acc: 93.99%\n",
            "Val Loss: 1.489670, Val Acc: 79.07%, Val Density: 0.5456\n",
            "-------------\n",
            "Epoch: 35\n",
            "Train Loss: 1.404, Train Acc: 94.25%\n",
            "Val Loss: 1.492296, Val Acc: 78.40%, Val Density: 0.5465\n",
            "-------------\n",
            "Epoch: 36\n",
            "Train Loss: 1.404, Train Acc: 94.25%\n",
            "Val Loss: 1.491283, Val Acc: 78.94%, Val Density: 0.5450\n",
            "-------------\n",
            "Epoch: 37\n",
            "Train Loss: 1.403, Train Acc: 94.43%\n",
            "Val Loss: 1.493702, Val Acc: 78.52%, Val Density: 0.5426\n",
            "-------------\n",
            "Epoch: 38\n",
            "Train Loss: 1.404, Train Acc: 94.19%\n",
            "Val Loss: 1.490361, Val Acc: 78.94%, Val Density: 0.5431\n",
            "-------------\n",
            "Epoch: 39\n",
            "Train Loss: 1.402, Train Acc: 94.48%\n",
            "Val Loss: 1.489895, Val Acc: 78.76%, Val Density: 0.5425\n",
            "-------------\n",
            "Epoch: 40\n",
            "Train Loss: 1.405, Train Acc: 94.01%\n",
            "Val Loss: 1.489676, Val Acc: 79.31%, Val Density: 0.5393\n",
            "-------------\n",
            "Epoch: 41\n",
            "Train Loss: 1.402, Train Acc: 94.59%\n",
            "Val Loss: 1.491508, Val Acc: 78.40%, Val Density: 0.5411\n",
            "-------------\n",
            "Epoch: 42\n",
            "Train Loss: 1.403, Train Acc: 94.43%\n",
            "Val Loss: 1.494752, Val Acc: 78.22%, Val Density: 0.5414\n",
            "-------------\n",
            "Epoch: 43\n",
            "Train Loss: 1.402, Train Acc: 94.64%\n",
            "Val Loss: 1.495835, Val Acc: 77.73%, Val Density: 0.5384\n",
            "-------------\n",
            "Epoch: 44\n",
            "Train Loss: 1.402, Train Acc: 94.53%\n",
            "Val Loss: 1.486631, Val Acc: 79.79%, Val Density: 0.5370\n",
            "-------------\n",
            "Epoch: 45\n",
            "Train Loss: 1.403, Train Acc: 94.43%\n",
            "Val Loss: 1.492560, Val Acc: 78.58%, Val Density: 0.5318\n",
            "-------------\n",
            "Epoch: 46\n",
            "Train Loss: 1.403, Train Acc: 94.35%\n",
            "Val Loss: 1.488888, Val Acc: 79.37%, Val Density: 0.4514\n",
            "-------------\n",
            "Epoch: 47\n",
            "Train Loss: 1.402, Train Acc: 94.43%\n",
            "Val Loss: 1.488575, Val Acc: 79.55%, Val Density: 0.4421\n",
            "-------------\n",
            "Epoch: 48\n",
            "Train Loss: 1.401, Train Acc: 94.67%\n",
            "Val Loss: 1.495630, Val Acc: 77.85%, Val Density: 0.4396\n",
            "-------------\n",
            "Epoch: 49\n",
            "Train Loss: 1.401, Train Acc: 94.64%\n",
            "Val Loss: 1.488670, Val Acc: 79.43%, Val Density: 0.4483\n",
            "-------------\n",
            "Epoch: 50\n",
            "Train Loss: 1.399, Train Acc: 95.06%\n",
            "Val Loss: 1.487746, Val Acc: 79.61%, Val Density: 0.4477\n",
            "-------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wMQ9iDkqOack",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_stats.to_csv(\"BiLSTM+BiLSTM_BS_16.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luT3ozewOacn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Print model's state_dict\n",
        "# print(\"Model's state_dict:\")\n",
        "# for param_tensor in model.state_dict():\n",
        "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# # Print optimizer's state_dict\n",
        "# print(\"Optimizer's state_dict:\")\n",
        "# for var_name in optim.state_dict():\n",
        "#     print(var_name, \"\\t\", optim.state_dict()[var_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6nSbEikLOacp",
        "colab_type": "code",
        "outputId": "2d031c21-f242-4565-bfea-84d61c97d349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "loaded_model = GANet(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, tau= tau, biDirectional_aux=True, biDirectional_backbone=True)\n",
        "loaded_model.load_state_dict(torch.load('BiLSTM+BiLSTM_BS_16'))\n",
        "loaded_model.eval()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GANet(\n",
              "  (word_embeddings): Embedding(6944, 100)\n",
              "  (auxiliary): AuxiliaryNet(\n",
              "    (mlp): Aux_MLP(\n",
              "      (ff_1): Linear(in_features=100, out_features=64, bias=True)\n",
              "      (tanh): Tanh()\n",
              "      (ff_2): Linear(in_features=64, out_features=32, bias=True)\n",
              "      (ff_3): Linear(in_features=32, out_features=1, bias=True)\n",
              "      (sigmoid): Sigmoid()\n",
              "    )\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              "  (backbone): BackboneNet(\n",
              "    (backbone_lstm): LSTM(100, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (mlp): MLP(\n",
              "    (ff_1): Linear(in_features=200, out_features=32, bias=True)\n",
              "    (tanh): Tanh()\n",
              "    (ff_2): Linear(in_features=32, out_features=1, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              "  (FF): Linear(in_features=200, out_features=6, bias=True)\n",
              "  (tanh): Tanh()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vwMLkn7s7cnC",
        "outputId": "cc504948-4420-4e4b-f345-8411a5dc590f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_loss, test_acc, density = eval_model(loaded_model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}, Density: {density:.4f} ')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.467, Test Acc: 83.59, Density: 0.5516 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kj2OOxjKaAIm",
        "outputId": "8e9654b0-4b3b-443f-9713-23290c7d2b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def test_sentence(test_sen):\n",
        "    test_sen_list = TEXT.preprocess(test_sen)\n",
        "    print(test_sen_list)\n",
        "    test_sen = [[TEXT.vocab.stoi[x] for x in test_sen_list]]\n",
        "    # print(test_sen)\n",
        "\n",
        "    test_sen = np.asarray(test_sen)\n",
        "    test_sen = torch.LongTensor(test_sen)\n",
        "    test_tensor = Variable(test_sen, volatile=True)\n",
        "\n",
        "    # print(test_tensor)\n",
        "    loaded_model.eval()\n",
        "    prediction, g_t, alpha, p_t = loaded_model(test_tensor, is_train = False)\n",
        "    print(\"prediction =\", prediction)\n",
        "#     print(\"g =\", g_t)\n",
        "    \n",
        "    for i in range(len(test_sen_list)):\n",
        "        print(test_sen_list[i], int(g_t[0][i]), float(p_t[0][i][0]), float(alpha[0][i][0]))\n",
        "    out_class = torch.argmax(prediction)\n",
        "    return out_class\n",
        "\n",
        "# ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n",
        "test_sen0 = \"What does the six-footed Musca domestica become when it enters a house ?\" # class = Entity - 0\n",
        "test_sen1 = \"Who killed Gandhi ?\"   # Class: HUM - 1\n",
        "test_sen2 = \"What does target heart rate mean ?\" # class = \"DESC\" - 2\n",
        "test_sen3 = \"How old was Joan of Arc when she died ?\" # class = \"NUM\" - 3\n",
        "test_sen4 = \"Where on the body is a mortarboard worn ?\" # class = \"LOC\" - 4\n",
        "test_sen5 = \"What does I.V. stand for ?\" # class = \"ABBR\" - 5\n",
        "x = test_sentence(test_sen0)\n",
        "print(x)\n",
        "print('------------')\n",
        "x = test_sentence(test_sen1)\n",
        "print(x)\n",
        "print('------------')\n",
        "x = test_sentence(test_sen2)\n",
        "print(x)\n",
        "print('------------')\n",
        "x = test_sentence(test_sen3)\n",
        "print(x)\n",
        "print('------------')\n",
        "x = test_sentence(test_sen4)\n",
        "print(x)\n",
        "print('------------')\n",
        "x = test_sentence(test_sen5)\n",
        "print(x)\n",
        "print('------------')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what', 'does', 'the', 'six-footed', 'musca', 'domestica', 'become', 'when', 'it', 'enters', 'a', 'house', '?']\n",
            "prediction = tensor([[0.5963, 0.0807, 0.0809, 0.0807, 0.0807, 0.0807]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "what 0 0.003976395819336176 0.0\n",
            "does 0 0.0021155120339244604 0.0\n",
            "the 0 0.0269416905939579 0.0\n",
            "six-footed 1 0.9900999665260315 0.11721692979335785\n",
            "musca 1 0.9900999665260315 0.15338003635406494\n",
            "domestica 1 0.9900999665260315 0.14226682484149933\n",
            "become 1 0.9969335794448853 0.1218630000948906\n",
            "when 1 0.9983291029930115 0.3299843668937683\n",
            "it 1 0.9951359629631042 0.11924856901168823\n",
            "enters 1 0.9900999665260315 0.01604028232395649\n",
            "a 0 0.008908516727387905 0.0\n",
            "house 0 0.0013946479884907603 0.0\n",
            "? 0 0.0009937817230820656 0.0\n",
            "tensor(0, grad_fn=<NotImplemented>)\n",
            "------------\n",
            "['who', 'killed', 'gandhi', '?']\n",
            "prediction = tensor([[0.0807, 0.5964, 0.0807, 0.0807, 0.0807, 0.0807]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "who 0 0.002099711913615465 0.0\n",
            "killed 1 0.9991368651390076 0.5644112825393677\n",
            "gandhi 1 0.43960022926330566 0.4355887174606323\n",
            "? 0 0.0009937817230820656 0.0\n",
            "tensor(1, grad_fn=<NotImplemented>)\n",
            "------------\n",
            "['what', 'does', 'target', 'heart', 'rate', 'mean', '?']\n",
            "prediction = tensor([[0.0807, 0.0807, 0.5964, 0.0808, 0.0807, 0.0807]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "what 0 0.003976395819336176 0.0\n",
            "does 0 0.0021155120339244604 0.0\n",
            "target 1 0.9984632730484009 0.20291002094745636\n",
            "heart 1 0.9991635084152222 0.3588559627532959\n",
            "rate 0 0.0023178746923804283 0.0\n",
            "mean 1 0.9990981817245483 0.43823403120040894\n",
            "? 0 0.0009937817230820656 0.0\n",
            "tensor(2, grad_fn=<NotImplemented>)\n",
            "------------\n",
            "['how', 'old', 'was', 'joan', 'of', 'arc', 'when', 'she', 'died', '?']\n",
            "prediction = tensor([[0.0807, 0.0807, 0.0807, 0.5964, 0.0807, 0.0807]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "how 1 0.9233803153038025 8.247062942245975e-05\n",
            "old 1 0.9970351457595825 0.15783455967903137\n",
            "was 1 0.9991673231124878 0.12524881958961487\n",
            "joan 0 0.3237246870994568 0.0\n",
            "of 0 0.0014710522955283523 0.0\n",
            "arc 1 0.9900999665260315 0.23717036843299866\n",
            "when 1 0.9983291029930115 0.25905561447143555\n",
            "she 1 0.99660325050354 0.2206081748008728\n",
            "died 0 0.024648748338222504 0.0\n",
            "? 0 0.0009937817230820656 0.0\n",
            "tensor(3, grad_fn=<NotImplemented>)\n",
            "------------\n",
            "['where', 'on', 'the', 'body', 'is', 'a', 'mortarboard', 'worn', '?']\n",
            "prediction = tensor([[0.0807, 0.0807, 0.0807, 0.0807, 0.5964, 0.0807]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "where 1 0.999180257320404 0.922254204750061\n",
            "on 0 0.0037055625580251217 0.0\n",
            "the 0 0.0269416905939579 0.0\n",
            "body 1 0.9976822137832642 0.07767870277166367\n",
            "is 0 0.0015610704431310296 0.0\n",
            "a 0 0.008908516727387905 0.0\n",
            "mortarboard 1 0.9900999665260315 4.087352499482222e-05\n",
            "worn 1 0.9900999665260315 2.6201851142104715e-05\n",
            "? 0 0.0009937817230820656 0.0\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "------------\n",
            "['what', 'does', 'i.v.', 'stand', 'for', '?']\n",
            "prediction = tensor([[0.5430, 0.0750, 0.1571, 0.0751, 0.0749, 0.0749]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "what 0 0.003976395819336176 0.0\n",
            "does 0 0.0021155120339244604 0.0\n",
            "i.v. 1 0.9900999665260315 1.0\n",
            "stand 0 0.0011671397369354963 0.0\n",
            "for 0 0.0025714440271258354 0.0\n",
            "? 0 0.0009937817230820656 0.0\n",
            "tensor(0, grad_fn=<NotImplemented>)\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DJf5OMBaO8Wl",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq49vAs5Oacz",
        "colab_type": "code",
        "outputId": "4dc29adf-c838-4ba1-c0ef-f9f5e86e1d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training_stats"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Train_Loss</th>\n",
              "      <th>Train_Acc</th>\n",
              "      <th>Val_Loss</th>\n",
              "      <th>Val_Acc</th>\n",
              "      <th>Val_Density</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.627947</td>\n",
              "      <td>52.588912</td>\n",
              "      <td>1.556285</td>\n",
              "      <td>65.716019</td>\n",
              "      <td>tensor(0.4880)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.526138</td>\n",
              "      <td>71.103556</td>\n",
              "      <td>1.527702</td>\n",
              "      <td>71.905340</td>\n",
              "      <td>tensor(0.4930)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.496183</td>\n",
              "      <td>76.412134</td>\n",
              "      <td>1.523074</td>\n",
              "      <td>72.512136</td>\n",
              "      <td>tensor(0.4616)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.481103</td>\n",
              "      <td>79.341004</td>\n",
              "      <td>1.515676</td>\n",
              "      <td>74.089806</td>\n",
              "      <td>tensor(0.4869)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.469833</td>\n",
              "      <td>81.668410</td>\n",
              "      <td>1.516554</td>\n",
              "      <td>73.665049</td>\n",
              "      <td>tensor(0.4902)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1.456722</td>\n",
              "      <td>84.126569</td>\n",
              "      <td>1.508507</td>\n",
              "      <td>75.182039</td>\n",
              "      <td>tensor(0.4530)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1.447627</td>\n",
              "      <td>86.114017</td>\n",
              "      <td>1.503547</td>\n",
              "      <td>76.395631</td>\n",
              "      <td>tensor(0.4620)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1.441346</td>\n",
              "      <td>87.212343</td>\n",
              "      <td>1.501201</td>\n",
              "      <td>77.123786</td>\n",
              "      <td>tensor(0.5057)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1.432639</td>\n",
              "      <td>88.833682</td>\n",
              "      <td>1.498095</td>\n",
              "      <td>77.912621</td>\n",
              "      <td>tensor(0.5100)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1.428836</td>\n",
              "      <td>89.670502</td>\n",
              "      <td>1.499159</td>\n",
              "      <td>77.305825</td>\n",
              "      <td>tensor(0.5139)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>1.424833</td>\n",
              "      <td>90.245816</td>\n",
              "      <td>1.494925</td>\n",
              "      <td>78.337379</td>\n",
              "      <td>tensor(0.5230)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>1.421524</td>\n",
              "      <td>91.056485</td>\n",
              "      <td>1.500166</td>\n",
              "      <td>77.002427</td>\n",
              "      <td>tensor(0.5268)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>1.419848</td>\n",
              "      <td>91.317992</td>\n",
              "      <td>1.495342</td>\n",
              "      <td>78.276699</td>\n",
              "      <td>tensor(0.5410)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>1.418055</td>\n",
              "      <td>91.631799</td>\n",
              "      <td>1.490486</td>\n",
              "      <td>78.944175</td>\n",
              "      <td>tensor(0.5358)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>1.418529</td>\n",
              "      <td>91.501046</td>\n",
              "      <td>1.491834</td>\n",
              "      <td>78.701456</td>\n",
              "      <td>tensor(0.5302)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>1.414421</td>\n",
              "      <td>92.337866</td>\n",
              "      <td>1.487510</td>\n",
              "      <td>79.490291</td>\n",
              "      <td>tensor(0.5267)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>1.414360</td>\n",
              "      <td>92.337866</td>\n",
              "      <td>1.490074</td>\n",
              "      <td>79.368932</td>\n",
              "      <td>tensor(0.5348)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>1.413817</td>\n",
              "      <td>92.416318</td>\n",
              "      <td>1.485773</td>\n",
              "      <td>79.611650</td>\n",
              "      <td>tensor(0.5240)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>1.414988</td>\n",
              "      <td>92.207113</td>\n",
              "      <td>1.487042</td>\n",
              "      <td>79.915049</td>\n",
              "      <td>tensor(0.5224)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>1.412029</td>\n",
              "      <td>92.808577</td>\n",
              "      <td>1.494529</td>\n",
              "      <td>77.973301</td>\n",
              "      <td>tensor(0.5224)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>1.411954</td>\n",
              "      <td>92.730126</td>\n",
              "      <td>1.490775</td>\n",
              "      <td>79.126214</td>\n",
              "      <td>tensor(0.5122)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>1.409313</td>\n",
              "      <td>93.253138</td>\n",
              "      <td>1.489731</td>\n",
              "      <td>79.672330</td>\n",
              "      <td>tensor(0.4970)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>1.409027</td>\n",
              "      <td>93.331590</td>\n",
              "      <td>1.496557</td>\n",
              "      <td>77.791262</td>\n",
              "      <td>tensor(0.5343)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>1.411692</td>\n",
              "      <td>92.808577</td>\n",
              "      <td>1.488464</td>\n",
              "      <td>79.429612</td>\n",
              "      <td>tensor(0.4674)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>1.408287</td>\n",
              "      <td>93.462343</td>\n",
              "      <td>1.491631</td>\n",
              "      <td>78.640777</td>\n",
              "      <td>tensor(0.5252)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>1.407228</td>\n",
              "      <td>93.697699</td>\n",
              "      <td>1.487495</td>\n",
              "      <td>79.672330</td>\n",
              "      <td>tensor(0.5505)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>1.405729</td>\n",
              "      <td>93.933054</td>\n",
              "      <td>1.486421</td>\n",
              "      <td>79.975728</td>\n",
              "      <td>tensor(0.5460)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>1.405625</td>\n",
              "      <td>93.959205</td>\n",
              "      <td>1.486361</td>\n",
              "      <td>80.157767</td>\n",
              "      <td>tensor(0.5474)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>1.405253</td>\n",
              "      <td>93.985356</td>\n",
              "      <td>1.490585</td>\n",
              "      <td>79.126214</td>\n",
              "      <td>tensor(0.5451)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>1.404697</td>\n",
              "      <td>94.142259</td>\n",
              "      <td>1.490200</td>\n",
              "      <td>79.065534</td>\n",
              "      <td>tensor(0.5432)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>1.404494</td>\n",
              "      <td>94.116109</td>\n",
              "      <td>1.493039</td>\n",
              "      <td>78.640777</td>\n",
              "      <td>tensor(0.5519)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>1.406111</td>\n",
              "      <td>93.880753</td>\n",
              "      <td>1.489063</td>\n",
              "      <td>78.883495</td>\n",
              "      <td>tensor(0.5435)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>1.405863</td>\n",
              "      <td>93.854603</td>\n",
              "      <td>1.489563</td>\n",
              "      <td>78.944175</td>\n",
              "      <td>tensor(0.5382)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>1.405305</td>\n",
              "      <td>93.985356</td>\n",
              "      <td>1.489670</td>\n",
              "      <td>79.065534</td>\n",
              "      <td>tensor(0.5456)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>1.403884</td>\n",
              "      <td>94.246862</td>\n",
              "      <td>1.492296</td>\n",
              "      <td>78.398058</td>\n",
              "      <td>tensor(0.5465)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>1.403589</td>\n",
              "      <td>94.246862</td>\n",
              "      <td>1.491283</td>\n",
              "      <td>78.944175</td>\n",
              "      <td>tensor(0.5450)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>1.402682</td>\n",
              "      <td>94.429916</td>\n",
              "      <td>1.493702</td>\n",
              "      <td>78.519417</td>\n",
              "      <td>tensor(0.5426)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>1.403807</td>\n",
              "      <td>94.194561</td>\n",
              "      <td>1.490361</td>\n",
              "      <td>78.944175</td>\n",
              "      <td>tensor(0.5431)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>1.402390</td>\n",
              "      <td>94.482218</td>\n",
              "      <td>1.489895</td>\n",
              "      <td>78.762136</td>\n",
              "      <td>tensor(0.5425)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>1.404673</td>\n",
              "      <td>94.011506</td>\n",
              "      <td>1.489676</td>\n",
              "      <td>79.308252</td>\n",
              "      <td>tensor(0.5393)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>1.401943</td>\n",
              "      <td>94.586820</td>\n",
              "      <td>1.491508</td>\n",
              "      <td>78.398058</td>\n",
              "      <td>tensor(0.5411)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>1.402808</td>\n",
              "      <td>94.429916</td>\n",
              "      <td>1.494752</td>\n",
              "      <td>78.216019</td>\n",
              "      <td>tensor(0.5414)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>1.401574</td>\n",
              "      <td>94.639121</td>\n",
              "      <td>1.495835</td>\n",
              "      <td>77.730583</td>\n",
              "      <td>tensor(0.5384)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>44</td>\n",
              "      <td>1.402316</td>\n",
              "      <td>94.534519</td>\n",
              "      <td>1.486631</td>\n",
              "      <td>79.793689</td>\n",
              "      <td>tensor(0.5370)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>45</td>\n",
              "      <td>1.402724</td>\n",
              "      <td>94.429916</td>\n",
              "      <td>1.492560</td>\n",
              "      <td>78.580097</td>\n",
              "      <td>tensor(0.5318)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>46</td>\n",
              "      <td>1.403256</td>\n",
              "      <td>94.351464</td>\n",
              "      <td>1.488888</td>\n",
              "      <td>79.368932</td>\n",
              "      <td>tensor(0.4514)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>47</td>\n",
              "      <td>1.402268</td>\n",
              "      <td>94.429916</td>\n",
              "      <td>1.488575</td>\n",
              "      <td>79.550971</td>\n",
              "      <td>tensor(0.4421)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>48</td>\n",
              "      <td>1.401336</td>\n",
              "      <td>94.665272</td>\n",
              "      <td>1.495630</td>\n",
              "      <td>77.851942</td>\n",
              "      <td>tensor(0.4396)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>49</td>\n",
              "      <td>1.401115</td>\n",
              "      <td>94.639121</td>\n",
              "      <td>1.488670</td>\n",
              "      <td>79.429612</td>\n",
              "      <td>tensor(0.4483)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>50</td>\n",
              "      <td>1.398941</td>\n",
              "      <td>95.057531</td>\n",
              "      <td>1.487746</td>\n",
              "      <td>79.611650</td>\n",
              "      <td>tensor(0.4477)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Epoch  Train_Loss  Train_Acc  Val_Loss    Val_Acc     Val_Density\n",
              "0      1    1.627947  52.588912  1.556285  65.716019  tensor(0.4880)\n",
              "1      2    1.526138  71.103556  1.527702  71.905340  tensor(0.4930)\n",
              "2      3    1.496183  76.412134  1.523074  72.512136  tensor(0.4616)\n",
              "3      4    1.481103  79.341004  1.515676  74.089806  tensor(0.4869)\n",
              "4      5    1.469833  81.668410  1.516554  73.665049  tensor(0.4902)\n",
              "5      6    1.456722  84.126569  1.508507  75.182039  tensor(0.4530)\n",
              "6      7    1.447627  86.114017  1.503547  76.395631  tensor(0.4620)\n",
              "7      8    1.441346  87.212343  1.501201  77.123786  tensor(0.5057)\n",
              "8      9    1.432639  88.833682  1.498095  77.912621  tensor(0.5100)\n",
              "9     10    1.428836  89.670502  1.499159  77.305825  tensor(0.5139)\n",
              "10    11    1.424833  90.245816  1.494925  78.337379  tensor(0.5230)\n",
              "11    12    1.421524  91.056485  1.500166  77.002427  tensor(0.5268)\n",
              "12    13    1.419848  91.317992  1.495342  78.276699  tensor(0.5410)\n",
              "13    14    1.418055  91.631799  1.490486  78.944175  tensor(0.5358)\n",
              "14    15    1.418529  91.501046  1.491834  78.701456  tensor(0.5302)\n",
              "15    16    1.414421  92.337866  1.487510  79.490291  tensor(0.5267)\n",
              "16    17    1.414360  92.337866  1.490074  79.368932  tensor(0.5348)\n",
              "17    18    1.413817  92.416318  1.485773  79.611650  tensor(0.5240)\n",
              "18    19    1.414988  92.207113  1.487042  79.915049  tensor(0.5224)\n",
              "19    20    1.412029  92.808577  1.494529  77.973301  tensor(0.5224)\n",
              "20    21    1.411954  92.730126  1.490775  79.126214  tensor(0.5122)\n",
              "21    22    1.409313  93.253138  1.489731  79.672330  tensor(0.4970)\n",
              "22    23    1.409027  93.331590  1.496557  77.791262  tensor(0.5343)\n",
              "23    24    1.411692  92.808577  1.488464  79.429612  tensor(0.4674)\n",
              "24    25    1.408287  93.462343  1.491631  78.640777  tensor(0.5252)\n",
              "25    26    1.407228  93.697699  1.487495  79.672330  tensor(0.5505)\n",
              "26    27    1.405729  93.933054  1.486421  79.975728  tensor(0.5460)\n",
              "27    28    1.405625  93.959205  1.486361  80.157767  tensor(0.5474)\n",
              "28    29    1.405253  93.985356  1.490585  79.126214  tensor(0.5451)\n",
              "29    30    1.404697  94.142259  1.490200  79.065534  tensor(0.5432)\n",
              "30    31    1.404494  94.116109  1.493039  78.640777  tensor(0.5519)\n",
              "31    32    1.406111  93.880753  1.489063  78.883495  tensor(0.5435)\n",
              "32    33    1.405863  93.854603  1.489563  78.944175  tensor(0.5382)\n",
              "33    34    1.405305  93.985356  1.489670  79.065534  tensor(0.5456)\n",
              "34    35    1.403884  94.246862  1.492296  78.398058  tensor(0.5465)\n",
              "35    36    1.403589  94.246862  1.491283  78.944175  tensor(0.5450)\n",
              "36    37    1.402682  94.429916  1.493702  78.519417  tensor(0.5426)\n",
              "37    38    1.403807  94.194561  1.490361  78.944175  tensor(0.5431)\n",
              "38    39    1.402390  94.482218  1.489895  78.762136  tensor(0.5425)\n",
              "39    40    1.404673  94.011506  1.489676  79.308252  tensor(0.5393)\n",
              "40    41    1.401943  94.586820  1.491508  78.398058  tensor(0.5411)\n",
              "41    42    1.402808  94.429916  1.494752  78.216019  tensor(0.5414)\n",
              "42    43    1.401574  94.639121  1.495835  77.730583  tensor(0.5384)\n",
              "43    44    1.402316  94.534519  1.486631  79.793689  tensor(0.5370)\n",
              "44    45    1.402724  94.429916  1.492560  78.580097  tensor(0.5318)\n",
              "45    46    1.403256  94.351464  1.488888  79.368932  tensor(0.4514)\n",
              "46    47    1.402268  94.429916  1.488575  79.550971  tensor(0.4421)\n",
              "47    48    1.401336  94.665272  1.495630  77.851942  tensor(0.4396)\n",
              "48    49    1.401115  94.639121  1.488670  79.429612  tensor(0.4483)\n",
              "49    50    1.398941  95.057531  1.487746  79.611650  tensor(0.4477)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0rW8bVIOac1",
        "colab_type": "code",
        "outputId": "726c5832-76f4-4171-bc31-692d59ce548a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(training_stats['Train_Loss'], label=\"Train\")\n",
        "plt.plot(training_stats['Val_Loss'], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVADpiSKOac3",
        "colab_type": "code",
        "outputId": "ac0e8444-b439-46ab-8c22-144c49bf3522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(training_stats['Train_Acc'], label=\"Train\")\n",
        "plt.plot(training_stats['Val_Acc'], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3yV5f3/8deVvRMSEgIJkLASVHYYLoYbUXEggrWFVmul/X6ttdZVrbt1fa3tr45iUVFRcKLgRAShzgSIzLBXAhlk77Ou3x/XSQgQQgg5OblzPs/Hg8fJuc+67nDyPtf9ua77OkprjRBCCOvx83YDhBBCtI0EuBBCWJQEuBBCWJQEuBBCWJQEuBBCWFRAR75Y9+7ddUpKSke+pBBCWN6aNWsOaa3jj97eoQGekpJCVlZWR76kEEJYnlJqb3PbpYQihBAWJQEuhBAWJQEuhBAWJQEuhBAWJQEuhBAWJQEuhBAWJQEuhBAWJQEuhBAeYne6+GbHIR5Zuhmbw9Xuz9+hJ/IIIURXV2NzsGpbEV9sKmB5TiHltXZCAv24akQSZyRFt+trSYALIbyqzu4kc08J0aGB9IoJJS48CKXUMfersTnYVVTNzqIqdhZV43JpuoUHERseSLewIGLDgxovw4L8m32OtiquqmdrfiU5+ZVsK6ikvNaOv58i0N+PAD9FgL8fgf6KA2W1rN5+iHqHi5iwQC4Y3IOLTu/B+IHxhAb5t1t7GkiACyGOUGd3sr+khr3FNewtqeFAWS09ooJJS4xicGIk8ZHBx4RjSbWN7P2lZO8rY93+Mvz9FFOG9OTiMxKJCgls9nUOltfyxvd7efOHfZTW2Bu3BwX4kRQTSq+YEHpEhlBUVc+uomryymob7+OnQCmF09X8N4oFBfgRGxZ0RMB3CwsiMiSAyJBA92UAUSGBhAX5U2t3UlnncP+zU1nnoKLOzq6ianLyKzlUVd/43HHhQcRFBOFwauwul7l0auxOF9Ghgcwc04eLTu/BmJRYAvw9W6VWHfmVahkZGVrWQhG+rKLOzoGyWspr7I29tgA/96W/H2FB/iQ0E5Cnos7uZMvBCjYeqGBTXjn7S2uavZ/N4WJ/SS35FXVHbA8O8KO+Sf22W1ggaYmRpPWIpKzWzrp9ZewrMc/ppyAtMYrqegf7SmoICvDjgsEJXDEsiUnp8QT5+7F2Xykvf7OHzzbmo7XmgsE9uG50b1waDpTVcqCsljz3v/zyOuIigugfH8GA+Aj6J0TQPz6CvnFhBPn7UVnnoKTGRkm1jdJq2+Gfa9zXq+2UNtlWWec4bug35acgIjiAvnHhpCVGkp4Y6b6MIj4y+BT+N9pGKbVGa51xzHYJcCFaprWmoKKeXUVV1Nqd2J0aR2PPy4XDpXE4XY3b7U6Nw/1zaY2NA2V1JpRKa6msd5zw9SKDA0xANgZHFIN6RBARHIC/nzom3LXWlNbYKayso7CinsLKegoq6thVVM2mA+VsL6xqDK1uYYGkdA/Hv5kPCH8/RVK3UPrGhtM3Low+cWH0jQ0jNjyI0ho7OfkVbM2vPKKUEBUSyPDeMQzvE8OI3jEMSY4mLCgArTXZ+8v4MPsAS9cf4FCVjciQAJJiQsnJryQqJIAZY/rw83F96R0b1j7/Ua2gtW7S27ZTUeegpt5JaJBfk555IOHtXII5VRLgQrRCVb2jMaS25leQ4w6r8lr7iR/cjOjQQHc5IJSkmBCSupmfu4UF4XBp7A7X4dB3uaiqc7CtoIoc92tX1h0b+Ef32ivr7Nidx/4dd48IZkhSFGckRTf+6xUd0uHB5HC6+HZnMYuz89hzqJqrRyZz9cgkwoKkgttaxwtw+Q0Kn+R0aXYVVZHTpEeZk19BbunhOmtEcACDekRw6ZCepCdGMjAhgrDgAAIaBq/8FUHuy6aB2nC7v9+pBaXWmoPldWzNr2RHYRU1NmeTHr7p+dudLqJCA0mIDCYhMoSEqODGnz0xaNYWAf5+jB8Uz/hBxyxnLU6RBLiwlMLKOkqr7UcMNFXWOaixORidEsvw3jEt9jDtTheL1+Xx3Iod7Ck2dVt/P0W/7uEM7x3DjNG9SUuMIj0xkuRuoV49jFZK0cvde5+UnuC1dojOSwJcdHrV9Q6Wrj/Awsz9rNtX1uJ903pEMmNMb64akURMWFDjdpvDxXtrc3luxQ5yS2s5rWcUT1wzhDOSohmQEEFwQOforQpxMqQGLrxGa43DpQlsZqpVwyDYosz9LPnpANU2JwMTIrhmVDK9u4U1TgOLDAkkKsQM7n2+qYBFmfv4KbecoAA/Ljk9ketG92ZXURUvrNzJgfI6hiVHc+v5AzkvPaFTDVIJ0RIZxBSdRq3Nybtrc3n5v7vZfaiakMAjZwBEhQRQUFHHtoIqQgP9uXxYT64b3YeRfVoujzTYfKCCRZn7+GBdHhXuQcBRfbtx6/kDGT+wuwS3sBwJcOERdXZnYx261uakR1QI3SOaP5OuqLKe177bwxvf76W0xs6w5GgmpiVQY3M0nkTR8FxBAX5cOTyJy4f1JPI4J4K0pm3LtxQSFxHE2NRYCW5hWTILRbRajc3BP5ZvZ9nmAmjm892pNVXuwLU5j12gJzo0kP7x4fR3n3jRNzaMlVuL+CA7D7vTxQWDe/Drc/sxOqWbR0M1JNCfKUN7euz5hfA2CXBxhBVbC7l/8UZyS2uZmBbfbO+34Sy1hrJHlLv0ERrkz4GyWrNWRWE1K7cV8c6aXMCczXftqGRuPCeVfvERHb1bQnRJEuACMNPzHl6ymaXrD9I/PpxFN49jbL+4U37e8lo7ew5V09t9Rp8Qov1IgHdxFXV21u4tZe2+MhS4T/QIoYf7MjY8iHfW7OfxT3Oot7u4/cJB/GZCv3abVhcdGsiw3jHt8lxCiCNJgHcx+eV1ZO4pIWtPCZl7SsnJr8ClTdmjpTV8zuwXx2NXnSHlDSEsRALcorTW5JXVsjGvgk0HytmYV87GAxUUVZplL8OC/BnZx0ydazhDMSjAj0NV9Y0LHjUsfjQgIYLLhvaUWRpCWIwEuMXUO5zM/XoXL3+zu3ENZX8/xcCECMYPjOeMpChG9e3G4J5RzZ4g0zM6lJ7RoR3dbCGEB0iAW8iqbUU88NEmdh+q5oLBCUxIS2BIUjTpiZGEBMqp4EL4GglwCzhYXsujS7fw8YaDpHYP57VfjZGV3YQQrQtwpdTvgV8DCnhJa/2sUioWWASkAHuA6VrrUg+10yfZnS5e/WYPf/9yG06X5o8XDuLmdpwhIoSwthMGuFLqDEx4jwFswGdKqaXAzcByrfXjSqm7gbuBuzzZWF9RWm1jYeZ+Xv9uDwfK6zgvPYEHLz+dPnEd980lQojOrzU98MHAD1rrGgCl1NfA1cBUYKL7PvOBlUiAn5Kc/Ape/WYPH6zLo97h4qz+cTx21RAmpsXLDBEhxDFaE+AbgceUUnFALXApkAX00FofdN8nH+jR3IOVUjdjeuv06dPnlBvcFa3aVsQLK3fy3a5iggP8uHpkErPOSiE9McrbTRNCdGInDHCt9Ral1BPAF0A1kA04j7qPVko1e5qI1nouMBfMaoSn3OIupLCijoeWbObjDQfpFR3CXZekM2N0b7rJKedCiFZo1SCm1noeMA9AKfVXIBcoUEr11FofVEr1BAo918yuxeXSLPhxH09+mkO908UdFw3i5vH9CQo4dt62EEIcT2tnoSRorQuVUn0w9e9xQCowC3jcffmhx1rZheTkV3DP+xtYt6+sscad2j3c280SQlhQa+eBv+eugduB32mty5RSjwNvK6VuBPYC0z3VyK5Aa83fl23j+ZU7iQoN5Jnpw7hqRJIMTgoh2qy1JZRzm9lWDJzf7i3qop5fuZN/frWDq0Ykcf9lp8nSqkKIUyZnYnaALzbl89TnW5k6vBfPTB8mvW4hRLuQUTMPy8mv4A+LshmWHM0T1wyV8BZCtBsJcA8qqbZx0/wswoMD+PfPM2TBKSFEu5ISiofYHC7mvLGGwsp63v7NmSRGh3i7SUKILkZ64B7y0JJN/LC7hCevGcpw+UoxIYQHSIB7wOvf7WHBD/u4ZUJ/rhyR5O3mCCG6KAnwdvbumlweXLKZ89MT+NPFad5ujhCiC5MaeDtxujR/+2QL//nvbs7qH8ezM4bj7yczToQQniMB3g7Ka+3c+tY6vt5WxOyzUvjzlMHNfh+lEEK0JwnwU7SrqIqbXstiX3ENf7t6CDPHyJK5QoiOIQF+ClZtK+J/3lxLgL8fC24ay9h+cd5ukhDCh0iAt9FnGw/y2wVrGdQjkpd+kUHvWPm6MyFEx5IAb4PCyjrufn8DQ5KiWfDrcUQEy69RCNHxZKTtJGmt+fMHG6mxOfm/6cMlvIUQXiMBfpIWZ+exbHMBf7oojQEJEd5ujhDCh0mAn4SCijoe+HATo/p241fnpHq7OUIIHycB3kpaa+55fwM2p4unpg2Vk3SEEF4nAd5K767J5aucQu68OJ1+8VI6EUJ4nwR4Kxwsr+XhJZsZkxLL7LNSvN0cIYQAJMBPSGvNXe9twOHSPHXtUPykdCKE6CQkwE/g7az9rNpWxN2T0+kbF+7t5gghRCMJ8BZU1Tt48rOtjE7pxs/H9fV2c4QQ4ggS4C2Yu2oXxdU2/jzlNCmdCCE6HQnw4yisrOM/q3cxZUhP+Uo0IUSnJAF+HP/4cjs2h0u+VUcI0WlJgDdjZ1EVCzP3c/3YPqR0l4FLIUTnJAHejKc+20pIgB+3nj/Q200RQojjkgA/ypq9pXy2KZ+bx/ene0Swt5sjhBDHJQHehNaaxz/dQveIYG46VxarEkJ0bhLgTXy5pZDMPaXcdsFAwmWdbyFEJycB7uZwunjisxz6dQ/nutG9vd0cIYQ4IQlwt3fX5LKjsIo7L0kj0F9+LUKIzk+SCqiud/DMsm2M6BPDxacners5QgjRKhLgwAsrd1JYWc/9l52GUnLKvBDCGnw+wPeX1DB39S6uHN6LkX26ebs5QgjRaj4f4I9/loOfgjsvSfd2U4QQ4qT4dID/uLuEj9cf5JYJ/ekVE+rt5gghxElpVYArpf6glNqklNqolHpLKRWilEpVSv2glNqhlFqklArydGPbk8uleXjpJnpFh/Cb8f293RwhhDhpJwxwpVQScCuQobU+A/AHZgBPAH/XWg8ASoEbPdnQ9vbumlw25lVw1+R0QoP8vd0cYVVOB1QcAJfL2y0RPqi1pxsGAKFKKTsQBhwEzgOud98+H3gQeKG9G+gJlXV2nvx8K6P6duOKYb283ZyuT2uw+uyeygLIWwMlO6FkN5TuNpfl+8HlgD5nwbWvQKRMQxUd54QBrrXOU0o9DewDaoEvgDVAmdba4b5bLpDU3OOVUjcDNwP06dOnPdp8yp5fuZNDVfXMm5Uh0wY9obYUcrNg/w+w73vIWwvDr4cpT3u7Za3jckLhZtP+/T+afSjbe/j2kBjolgK9hsPpV0FgGPz3GXjxXBPiKed4rek+r3gnlO6BAed7uyUd4oQBrpTqBkwFUoEy4B3gkta+gNZ6LjAXICMjQ7etme1nX3EN81bv5uqRSQyTb9ppmdMB/id4i9SWmbDL3wj56yE3E4pyzG3KHxLPgKSRkPkSDLgA0lr91jnSoe2Q9TLsXgVX/ds8b3vTGr79f7DqKaivMNvCE6DPWBhzMySPhvhBENrMdNPBl8GiG2D+FXDBA3DWrdY/6rAaWw28cY05Opp0H4y/o23/B/Za8yFQcQB6j4XgiJN7vNZQXXTkkVrpbpj8RPPvnVPQmhLKBcBurXURgFLqfeBsIEYpFeDuhScDee3aMg/56ydbCPBX3CXTBo+vYDN8eifsWQ3B0RCRYP6Fx5vLwDA4tM2Edvm+w48LjYXkDBgyzbzxe400b35HPcydBB/9L/z2ewiPa107nHbI+Riy5png9guEgBB470a4eSUEnmDmkNaw7C/gHwjn/AGCI49/37pyWPxbyFkKAy+CM6aZ4I7p27oQSBgMv14BH/2Pec39P8KVz0NIdOv21Rc46qG+EsK7e+b5V/7NBGXqBFjxKFQVwOQnwa+Fob7aUlj7GhRtPRy0lQcP3957LMxaCgGtmKOx8X1Y/Yx5DltVkxsURCdDVZFXAnwfME4pFYYpoZwPZAErgGnAQmAW8GG7tswDNuaV89mmfP544SB6RIV4uzmdT22Z+SP48SUIiYKzf296I1WFpkdRsBF2Fpk3Z/eB0HsMZPwSEodAj9MhsmfzYRcQDFf/24T4x3+Aa+e3HIo1JfD98+YPq6oAovvAeffDyF9A/gZ442pY9gBc+mTL+7P6afj2n+bndQvgokdgyLXHvnb+Rnj751C6Fy56DM78Xdt6biFRZt++fwGW3Q9zJ8LEeyE0xnzoBYVDUAQEhZkPu0APvgcd9eAXAH6dYIC+ZDeseRXWvQE1xXD2reb30p77n7cWvvsXjJwFlz0LX/7FHE3VHDJHbAFHre3vcsG612H5Q6ZNkT2hWyr0P8+Ux7qlmu2f3QVf/Bkufarl19+9Gt7/NcSnw4ifQ2yqeY7YVIjpc+zrt5PW1MB/UEq9C6wFHMA6TEnkY2ChUupR97Z5HmlhO/r3ql1EBgcw++wUbzelY9mqYcM7gDKlh/jBJkQauFyQ/QZ86X4zZ/zSBGZYbPPP15ZBycQhMOle8wez4V0Yem3z9yveCQummT/6gRfB6BtN6aUhiAacD+N+awJ+4IXmX3NyPoGvHoWh15nyxyd/Mn9gWS+bXlnPoeZ+Py2EJbeZnvLspdD3rJPbr6MpBWf+FnqNgHdmw/s3NX+/gBDof74pvQy65Pi/a4D6KnPEU1dmygS2arBXm0tbtfk/qyp0f9AWmp5efbkJpXNuNx98JwrL/A3mKKfPOHPkdKL/3/Jc2PaZKbPFpprQi+l7+HVcTtj2uTl62rHcPF/apRAcBd/8A7Z9AVe9aMYRTpXTbo7uwhPgwodNj/uiR831ZfebDsGMBYePwHLXwCd3wIG10OdME86JQ46zn/vNB0NSBgy7rvn7HNphymex/WH2x+YDu4MorTuuLJ2RkaGzsrI67PWa2l9Sw8SnV3LTOancc+lgr7Shw9mqTWB98w/Tg26g/MybrcfpkHCa+UM8sBZ6jzO92p7DPNMelxNemWxq5HO+g+ijxr33fgcLrzd/7DPeMiWM5tjr4KXzzD7N+RYi4o+8vXAL/OcCc5Twy09NqaXxQ+pBc9g86pegXbDmFeh7Dkx7GSJ7tO/+2qqhbJ87aKvclzXm58ItplxTkWfGClLOgcGXmw+Q0j3mqKBgAxRsMh9mtPB3GhJtwqqx1OUud+38CvZ9e/wgt9fB5sWQOQ9yfzy8PSoJ0i8zHy59zjo8DlK0DXKWwJYlcGBdMw1RENXL9DxL90BFrnntkbPMazf8f2/7wgRuzSEYfyece7spc7XVqqfMh/WMNyF9ypG3Zb8FH/7OdFyufBG+f84cCUQkHv+IrCmnA16bamYg3bTs2KCvKTHvtboyuGm5+TDzAKXUGq11xjHbfSXAH1qyide/28vquybRM7qLn3V5dHD3mwgT7jYBVbDJHQ7uf6V7IKIHXPgIDJ3u+YG34p3w4jmmtvjzDw6/3oZ3YfEcc7j5s3cgtl/Lz1Ow2ZQo+k+CmQsPP09NiQl3W7Wpkx/9IVFbCiv+ZgZVtcuUic77y4kHaz1Ba/PBuWWpCcXi7U1uVOZ3kHgG9DjDfNCGxZkjp6AIU45pKMscr0yitelVr3zcHeS9TFimnAPZC0xZqbYE4gZAxq9MD3nf96YtO5eDo86UevpNNO+bQ1vN8yaNcgf85ebDo2S3eR81HbALjjKhnTa5+XCuKTFHRRvfNUcrV74ICW0Ylyraat5P6VPg2lebv8+2z+HtWeCoNeMo4+bAhDtbHhNpqqoQ/j3B1MFvXnm4ju2wmXLe/h9g1hJz9OIhPh3g5TV2znx8OZeckcgz09vhkK2jrH/H/FFPuKt1tUyX09Rfv3n2yODue+bxH1NfCf7BrRukaS+Z8+Dj2+HSp2H0TbD6/+CrR0xvb8aClssJTX3/oqlRTvk/8zxOhym/7P3GHMr2HnP8xxbmmNJDytnts0/toWir6dnGDTCDokHh7fO8Rwc5mPp4+hQT3KkTjv3gtlXDji/Nh8uulSZc0y83jzn6Q/FUbPoAlt5uZv3E9jtcN264jO1njhabG4h0ueCVS0x56Xc/mqOP49mfaY7Axv3OzCQ6WfszzdFj/0kwc5H5fX30P6Y3f9Xc45dX2olPB/hzK3bw1Odb+ey2c0lPjOrw1z9p9jozC2TtfHN9yHRTL2wpxJ12+OAW06NJnQAT72k5uL1JazPda++3poe26X1zKDv1uZMb7Gn6PL/5GtbMN4fIV/wLRv7cc+23qoYgL9wCp1/ZeU46qiqEH140QVzi7sXbqw/fHt3HjJkMnXFk+P4wFz79k+m9D5/p+XZm/gc+/qPpFAWFmdlG4/8E593n8Zf22QCvdzg554kVDO4ZxWu/aqFH5mklu+G75yCuvxmlPt7c0tI98PYv4OBPpm4ZFGbqey2FuKMe3vklbP0Yzn/AHCZ3dhUH4PkzTe1w/J1mgLMt5ZvKfHjhLHNoXJUPY28x822FdTWdR31oK2xaDLtWmJJXr5EwbIY5unpliilb3PBex8y519pMNf3pTUCZD8FrXm55mmI7OV6Ad/lv7l28Lo+iynqeve4ENVVPsdXAf/9u6tHaBS67maqXcSOM/c2RvaBtX5iZElqbum7aZPcNypQY4NgQt9XAop+ZAavJT5rntIKoXnDD+2Yga9DFbX+eyETT4144E1LHm9kHwtqUOjwg22esqaVX5ptxkvULzdEpQGA4XP5sx50wpRRc9owpa/oFwJUvdEh4t9ikrtwDd7k0Fz27iiB/Pz6+9ZyOPW1ea9j8IXxxn5mKNORaM8WpPM/MTd6yxAzuDJ1upsVtWgyrnjSj3NNfO3YQb9XTJsSb9sTrKuDN62D/93D5P327bJCbZebgnuxZc8J6CjabUmHSqGNnnXQElxNQHRrePtkDX7G1kB2FVfxjxvCODe/CHNNL2P21mUFw1b8PD5ZF9YLrXjezMb5/3swEWPeGuW34DWa9kObOMBx/h7ls6Ilf8jd4c7optVzzHzjjGs/vV2eWfMx7W3RVPU6DHn/x3ut3hpOj3Lp0gM9dtYukmFAuHdKzY16wqhC+fsKcdRYUbmZZjPpl81PU4vqb2RMT74W1r5q5t8NmtPz8TUN866fgrIfpr0P6pe29J0IIC+iyAf7T/jJ+2F3CfVMGE+jv4UOd+iozQPntP82p56Nmm0G51qz5EB4H5/6x9a81/g5zIs53/4LrXjOn/gohfFKXDfC5q3YRGRLAjDEeXMLW6TDrKaz8m1mzY/DlZhZI94Gee00ws0zO+YOsdieEj+uSAb6/pIZPNx7kNxP6ExHsoV3MXQOLbzFzV3uPM6WM45367QkS3kL4vC4Z4F9uKcCl4XpP9b63LzNztcO7w3ULzEi4BKoQooN1yQD/flcxvWND6R0bduI7n6yfFsGHvzVrU9zwXsun7wohhAd5dxa6B7hcmh92lzAutZVfGnAyvv0XfHCzWTFu9scS3kIIr+pyPfCc/ErKauyc2b8dA1xr+PIBczblaVPh6pc8tkC7EEK0VpcL8O93FQMwtl87BbjTAUtuNctvZtxoFn/vRBP5hRC+q0sGeJ/YMJJi2mHNb5fLfKPKpg/M6n4T7pLBSiFEp9GlauAN9e8z26v3/cV9JrwvfAQm3i3hLYToVLpUgG/Jr6C81s64/q38QoCWfPe8WVt67C1w1v+e+vMJIUQ761IB/v2uEgDGnWoPfPOH8Pm95szKi/8qPW8hRKfUpQL8u53FpMSFndp3Xu77Ht77tVkw/uqXZMBSCNFpdZkAd7o0P+4ubrn3XbYfXjwX3roest80X6za1KHt8NYMiE4234re3LKuQgjRSXSZWShbDlZQUec4/vzv+krz5Qdl+8yX2W79GJS/+YbuwZebb0lfdIPZdsO7ZpVAIYToxLpMgDfO/27uDEyXE969EYpy4GfvmCVYD6w134qzZSl84l5nOyDUnGF59LfhCCFEJ9SlAjy1eziJ0SHH3vjFfbD9c/MFCgPON9uSRpl/FzwIRVth6yeQPBqSR3Vks4UQos26RIA73fO/Lxva69gbM+eZry4bOwdG39T8E8SnmX9CCGEhXWIQc/OBCirrHIzrd9T8751fwSd/goEXwcWPeadxQgjhIV0iwBvq30ecgVm0Fd6ebb6pfNrLMh1QCNHldIkA/25XMf3iw0mIcte/89aab2wPCIbrF0JwpHcbKIQQHmD5GrjD6SJzdwlXDEuEbZ/DN/+Evf+FkGi44X2I8eB3YgohhBdZPsC37C9ksmMZd+/+CtbvhKhkuOgxGPkLCInydvOEEMJjrB3g+zPp/+Z1PBlYjD3kdLjwJTj9KvAP9HbLhBDC46xdA1/zCtpRz59CHyLwt9/A0OkS3kIIn2HpANe5WWQ5BxE06HxZMVAI4XOsG+C1ZahDW8l09G/f778UQgiLsG6AH1gLQLYe0Pz6J0II0cVZN8Bz1wCwI2Ag8ZHyDfFCCN9j3VkoeVkUBPXB36+bt1sihBBeccIeuFIqTSmV3eRfhVLqNqVUrFJqmVJqu/uy45JUa8jNYnvQYGLCZNaJEMI3nTDAtdZbtdbDtdbDgVFADfABcDewXGs9EFjuvt4xyvZCzSE2qYES4EIIn3WyNfDzgZ1a673AVGC+e/t84Mr2bFiLcrMAWOPsT0xYUIe9rBBCdCYnG+AzgLfcP/fQWh90/5wP9GjuAUqpm5VSWUqprKKiojY28yi5WRAQSnZdL2JCpQcuhPBNrQ5wpVQQcAXwztG3aa01oJt7nNZ6rtY6Q2udER8f3+aGHiEvC91rOMV1LrpJD1wI4aNOpgc+GVirtS5wXy9QSvUEcF8WtnfjmuWwwcH12BJH4nRpqYELIXzWyQT4TA6XTwA+Ama5f54FfNhejWpRwQZw1lMZNxxAauBCCJ/VqgBXSoUDFwLvN9n8OHChUmo7cIH7uue5BzCLos8AkBq4EMJntepEHq11NRB31LZizKyUjpWbBRGJFKnuwG66ha1ESywAAA5jSURBVEuACyF8k/VOpc/LguQMSmvtAESHSglFCOGbrBXgNSVQsguSMyh3B7gMYgohfJW1AjzPLGBFUgal1e4Alxq4EMJHWSvAczNB+UGvEZTV2ogMDiDA31q7IIQQ7cVa6ZebBfGDITiCsho7MTKAKYTwYdYJcK1NCSU5A4CyGhsxMoAphPBh1gnw4p1QV3Y4wGvtMoAphPBp1gnwPHMCD0kNPXC7nIUphPBp1gnw3EwIioD4NMCUULpJD1wI4cMsFOBZkDQS/PxxuTTltXaZQiiE8GnWCHB7LRRsbCyfVNY5cGmIlhKKEMKHWSPAD64Hl6NxALO0xgYgJRQhhE+zRoDnZprLpMMzUEBOoxdC+DZrBHheFkT3hkjzrW0NPXCZhSKE8GWtWk7W6/pNhKRRjVfLa2QdFCGEsEaAj5p9xNXDNXDpgQshfJc1SihHKauxoxRESQ9cCOHDLBng5bV2okIC8fdT3m6KEEJ4jSUDvLTGJjNQhBA+z5IBXlYjZ2EKIYRFA9wmUwiFED7PmgEuS8kKIYQ1A7y02iZTCIUQPs9yAe50aSrqHERLDVwI4eMsF+Dl7nVQZCErIYSvs1yAl8k6KEIIAVgwwEtrZCVCIYQACwZ4ea30wIUQAiwY4GWyEqEQQgAWDPCGEopMIxRC+DrLBXh5jQ0/BZEh1lgJVwghPMVyAV5aYyc6NBA/WYlQCOHjLBfg5jR6KZ8IIYT1AlyWkhVCCMCSAS5LyQohBFgwwEtlKVkhhAAsGODlNbKUrBBCQCsDXCkVo5R6VymVo5TaopQ6UykVq5RappTa7r7s5unG2p0uKusdxIRKD1wIIVrbA/8H8JnWOh0YBmwB7gaWa60HAsvd1z2qcSXCcOmBCyHECQNcKRUNjAfmAWitbVrrMmAqMN99t/nAlZ5qZIOG0+hlLXAhhGhdDzwVKAJeUUqtU0r9RykVDvTQWh903ycf6NHcg5VSNyulspRSWUVFRafU2IalZOU0eiGEaF2ABwAjgRe01iOAao4ql2itNaCbe7DWeq7WOkNrnREfH39KjS2TpWSFEKJRawI8F8jVWv/gvv4uJtALlFI9AdyXhZ5p4mGl0gMXQohGJwxwrXU+sF8plebedD6wGfgImOXeNgv40CMtbKJhEDNaeuBCCEFrl/T7X2CBUioI2AX8EhP+byulbgT2AtM908TDSmts+PspIoNlJUIhhGhVEmqts4GMZm46v32b07KG0+iVkpUIhRDCUmdiltXYpXwihBBulqpFlNXaZABTiE7CbreTm5tLXV2dt5vSZYSEhJCcnExgYOs6qtYK8Bo7iVEh3m6GEALIzc0lMjKSlJQUKWu2A601xcXF5Obmkpqa2qrHWK6EIisRCtE51NXVERcXJ+HdTpRSxMXFndQRjcUCXL7MQYjORMK7fZ3s79MyAW5zuKi2OekmAS6EEICFArys1pyFGS0lFCEEUFxczPDhwxk+fDiJiYkkJSU1XrfZbC0+Nisri1tvvbWDWuo5lhnEbFwHRVYiFEIAcXFxZGdnA/Dggw8SERHBHXfc0Xi7w+EgIKD5iMvIyCAjo7lTW6zFcgEu0wiF6HweWrKJzQcq2vU5T+sVxQOXn35Sj5k9ezYhISGsW7eOs88+mxkzZvD73/+euro6QkNDeeWVV0hLS2PlypU8/fTTLF26lAcffJB9+/axa9cu9u3bx2233WaZ3rllArxhISsZxBRCtCQ3N5dvv/0Wf39/KioqWL16NQEBAXz55Zfce++9vPfee8c8JicnhxUrVlBZWUlaWhpz5sxp9Vxsb7JMgJfLUrJCdFon21P2pGuvvRZ/f38AysvLmTVrFtu3b0cphd1ub/YxU6ZMITg4mODgYBISEigoKCA5Obkjm90mlhnEPNwDlxKKEOL4wsPDG3++//77mTRpEhs3bmTJkiXHnWMdHBzc+LO/vz8Oh8Pj7WwPlgnwslo7gf6K8CB/bzdFCGER5eXlJCUlAfDqq696tzEeYJ0Ar7ETHRokJw4IIVrtzjvv5J577mHEiBGW6VWfDGW+Da1jZGRk6KysrDY9ds4ba9hRWMWy2ye0c6uEEG2xZcsWBg8e7O1mdDnN/V6VUmu01sfMe7RUD1wGMIUQ4jDLBHhpjY3oUBnAFEKIBpYJ8PJau6yDIoQQTVgmwEtlJUIhhDiCJQK8zu6kzu6SOeBCCNGEJQK8TM7CFEKIY1gjwN1LycpCVkKIBpMmTeLzzz8/Ytuzzz7LnDlzmr3/xIkTaZjGfOmll1JWVnbMfR588EGefvrpFl938eLFbN68ufH6X/7yF7788suTbX67sESAl1bLUrJCiCPNnDmThQsXHrFt4cKFzJw584SP/eSTT4iJiWnT6x4d4A8//DAXXHBBm57rVFliMavyWlkHRYhO7dO7IX9D+z5n4hCY/Phxb542bRr33XcfNpuNoKAg9uzZw4EDB3jrrbe4/fbbqa2tZdq0aTz00EPHPDYlJYWsrCy6d+/OY489xvz580lISKB3796MGjUKgJdeeom5c+dis9kYMGAAr7/+OtnZ2Xz00Ud8/fXXPProo7z33ns88sgjXHbZZUybNo3ly5dzxx134HA4GD16NC+88ALBwcGkpKQwa9YslixZgt1u55133iE9Pf2Uf0WW6IFLDVwIcbTY2FjGjBnDp59+Cpje9/Tp03nsscfIyspi/fr1fP3116xfv/64z7FmzRoWLlxIdnY2n3zyCZmZmY23XX311WRmZvLTTz8xePBg5s2bx1lnncUVV1zBU089RXZ2Nv3792+8f11dHbNnz2bRokVs2LABh8PBCy+80Hh79+7dWbt2LXPmzDlhmaa1LNEDL5UAF6Jza6Gn7EkNZZSpU6eycOFC5s2bx9tvv83cuXNxOBwcPHiQzZs3M3To0GYfv3r1aq666irCwsIAuOKKKxpv27hxI/fddx9lZWVUVVVx8cUXt9iWrVu3kpqayqBBgwCYNWsWzz33HLfddhtgPhAARo0axfvvv3/K+w5W6YHX2ggK8CM0UFYiFEIcNnXqVJYvX87atWupqakhNjaWp59+muXLl7N+/XqmTJly3CVkT2T27Nn861//YsOGDTzwwANtfp4GDUvWtudytdYI8Go7MaGBshKhEOIIERERTJo0iV/96lfMnDmTiooKwsPDiY6OpqCgoLG8cjzjx49n8eLF1NbWUllZyZIlSxpvq6yspGfPntjtdhYsWNC4PTIyksrKymOeKy0tjT179rBjxw4AXn/9dSZM8Ozie9YI8FqbTCEUQjRr5syZ/PTTT8ycOZNhw4YxYsQI0tPTuf766zn77LNbfOzIkSO57rrrGDZsGJMnT2b06NGNtz3yyCOMHTuWs88++4gBxxkzZvDUU08xYsQIdu7c2bg9JCSEV155hWuvvZYhQ4bg5+fHLbfc0v473IQllpN9bsUOKusc3D351EdthRDtQ5aT9YyTWU7WEoOYv5s0wNtNEEKITscSJRQhhBDHkgAXQrRZR5ZgfcHJ/j4lwIUQbRISEkJxcbGEeDvRWlNcXExISEirH2OJGrgQovNJTk4mNzeXoqIibzelywgJCSE5ObnV95cAF0K0SWBgIKmpqd5uhk+TEooQQliUBLgQQliUBLgQQlhUh56JqZQqAva28eHdgUPt2ByrkP32Lb663+C7+96a/e6rtY4/emOHBvipUEplNXcqaVcn++1bfHW/wXf3/VT2W0ooQghhURLgQghhUVYK8LneboCXyH77Fl/db/DdfW/zflumBi6EEOJIVuqBCyGEaEICXAghLMoSAa6UukQptVUptUMpdbe32+MpSqmXlVKFSqmNTbbFKqWWKaW2uy+7ebONnqCU6q2UWqGU2qyU2qSU+r17e5fed6VUiFLqR6XUT+79fsi9PVUp9YP7/b5IKdUlv09QKeWvlFqnlFrqvt7l91sptUcptUEpla2UynJva/P7vNMHuFLKH3gOmAycBsxUSp3m3VZ5zKvAJUdtuxtYrrUeCCx3X+9qHMAftdanAeOA37n/j7v6vtcD52mthwHDgUuUUuOAJ4C/a60HAKXAjV5soyf9HtjS5Lqv7PckrfXwJnO/2/w+7/QBDowBdmitd2mtbcBCYKqX2+QRWutVQMlRm6cC890/zweu7NBGdQCt9UGt9Vr3z5WYP+okuvi+a6PKfTXQ/U8D5wHvurd3uf0GUEolA1OA/7ivK3xgv4+jze9zKwR4ErC/yfVc9zZf0UNrfdD9cz7Qw5uN8TSlVAowAvgBH9h3dxkhGygElgE7gTKttcN9l676fn8WuBNwua/H4Rv7rYEvlFJrlFI3u7e1+X0u64FbiNZaK6W67LxPpVQE8B5wm9a6wnTKjK6671prJzBcKRUDfACke7lJHqeUugwo1FqvUUpN9HZ7Otg5Wus8pVQCsEwpldP0xpN9n1uhB54H9G5yPdm9zVcUKKV6ArgvC73cHo9QSgViwnuB1vp992af2HcArXUZsAI4E4hRSjV0rrri+/1s4Aql1B5MSfQ84B90/f1Ga53nvizEfGCP4RTe51YI8ExgoHuEOgiYAXzk5TZ1pI+AWe6fZwEferEtHuGuf84Dtmitn2lyU5fed6VUvLvnjVIqFLgQU/9fAUxz363L7bfW+h6tdbLWOgXz9/yV1vpndPH9VkqFK6UiG34GLgI2cgrvc0uciamUuhRTM/MHXtZaP+blJnmEUuotYCJmeckC4AFgMfA20AezFO90rfXRA52WppQ6B1gNbOBwTfReTB28y+67UmooZtDKH9OZeltr/bBSqh+mZxoLrANu0FrXe6+lnuMuodyhtb6sq++3e/8+cF8NAN7UWj+mlIqjje9zSwS4EEKIY1mhhCKEEKIZEuBCCGFREuBCCGFREuBCCGFREuBCCGFREuBCCGFREuBCCGFR/x9a/qtZ125BBQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTtmr4FcOac6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4neQrm3BOac8",
        "colab_type": "code",
        "outputId": "60412993-c11b-404d-86d2-190b0587b188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "t = torch.rand(2,3,4)\n",
        "t"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2110, 0.6173, 0.4820, 0.6794],\n",
              "         [0.2456, 0.4800, 0.4455, 0.1892],\n",
              "         [0.9780, 0.4053, 0.1661, 0.5889]],\n",
              "\n",
              "        [[0.4965, 0.2578, 0.3643, 0.2801],\n",
              "         [0.6266, 0.2672, 0.8421, 0.2898],\n",
              "         [0.4596, 0.7725, 0.8926, 0.2083]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPf8PP1YOac-",
        "colab_type": "code",
        "outputId": "914928bd-ea31-4c4e-f514-5d396ca0036b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "torch.exp(t)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.2350, 1.8538, 1.6194, 1.9726],\n",
              "         [1.2784, 1.6160, 1.5613, 1.2083],\n",
              "         [2.6591, 1.4998, 1.1807, 1.8019]],\n",
              "\n",
              "        [[1.6429, 1.2941, 1.4395, 1.3233],\n",
              "         [1.8713, 1.3063, 2.3213, 1.3362],\n",
              "         [1.5834, 2.1651, 2.4414, 1.2316]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JzEAGTLKOadA",
        "colab_type": "code",
        "outputId": "b9887120-b16e-49c3-dd11-d4b97858656e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "torch.sum(torch.exp(t), dim=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5.1724, 4.9697, 4.3613, 4.9829],\n",
              "        [5.0976, 4.7655, 6.2023, 3.8911]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP-MIxvhOadD",
        "colab_type": "code",
        "outputId": "17b3dd52-993d-42b0-b41e-6599b60d1253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "torch.sum(torch.exp(t), dim=1).repeat(3,1,1).transpose(0,1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[5.1724, 4.9697, 4.3613, 4.9829],\n",
              "         [5.1724, 4.9697, 4.3613, 4.9829],\n",
              "         [5.1724, 4.9697, 4.3613, 4.9829]],\n",
              "\n",
              "        [[5.0976, 4.7655, 6.2023, 3.8911],\n",
              "         [5.0976, 4.7655, 6.2023, 3.8911],\n",
              "         [5.0976, 4.7655, 6.2023, 3.8911]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZgmAKDpOadF",
        "colab_type": "code",
        "outputId": "04e612a3-4b80-4c7c-b5cb-b4a5db11170a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "torch.exp(t)/torch.sum(torch.exp(t), dim=1).repeat(3,1,1).transpose(0,1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2388, 0.3730, 0.3713, 0.3959],\n",
              "         [0.2472, 0.3252, 0.3580, 0.2425],\n",
              "         [0.5141, 0.3018, 0.2707, 0.3616]],\n",
              "\n",
              "        [[0.3223, 0.2716, 0.2321, 0.3401],\n",
              "         [0.3671, 0.2741, 0.3743, 0.3434],\n",
              "         [0.3106, 0.4543, 0.3936, 0.3165]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a36Jxzu1OadH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# g_t = torch.tensor([[[1.],\n",
        "#          [0.],\n",
        "#          [1.],\n",
        "#          [0.],\n",
        "#          [1.],\n",
        "#          [0.],\n",
        "#          [1.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.]],\n",
        "\n",
        "#         [[0.],\n",
        "#          [1.],\n",
        "#          [1.],\n",
        "#          [1.],\n",
        "#          [0.],\n",
        "#          [1.],\n",
        "#          [1.],\n",
        "#          [0.],\n",
        "#          [1.],\n",
        "#          [1.]],\n",
        "\n",
        "#         [[0.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.],\n",
        "#          [0.]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P7Nk392OadJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# g_t.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZJV3lIVOadL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# g_t_sum = g_t.sum(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFH8hDBSOadN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# g_t_sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4idx3hPOadP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = (g_t_sum == 0).nonzero()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKf966QYOadR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = x[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5nkdOOFOadS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FUWpAljOadU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in x:\n",
        "#     print(\"Old g\", g_t[i,:,:])\n",
        "#     new_g = torch.ones(g_t[i,:,:].shape)\n",
        "#     g_t[i,:,:] = new_g\n",
        "#     print(\"New g\", g_t[i,:,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPFl6aagOadW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au8PaaokOadY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEFEl5M3OadZ",
        "colab_type": "code",
        "outputId": "493a61c4-9ee4-4bb5-e921-2943554642be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "logits = torch.rand(3, 2, 1)\n",
        "print(logits)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.7669],\n",
            "         [0.0441]],\n",
            "\n",
            "        [[0.4030],\n",
            "         [0.8371]],\n",
            "\n",
            "        [[0.2873],\n",
            "         [0.9665]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzsBYHkdOadb",
        "colab_type": "code",
        "outputId": "80c9ef70-2d56-4d6f-a91c-4e56f8ae2f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "logits = logits.repeat(1,1,2)\n",
        "logits[:,:,0] = 1 - logits[:,:,0] \n",
        "print(logits)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.2331, 0.7669],\n",
            "         [0.9559, 0.0441]],\n",
            "\n",
            "        [[0.5970, 0.4030],\n",
            "         [0.1629, 0.8371]],\n",
            "\n",
            "        [[0.7127, 0.2873],\n",
            "         [0.0335, 0.9665]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggom64wlOadg",
        "colab_type": "code",
        "outputId": "6f9d7e4b-b1ed-462e-c6e4-ffd6c7f350c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Sample soft categorical using reparametrization trick:\n",
        "F.gumbel_softmax(logits, tau=1, hard=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4426, 0.5574],\n",
              "         [0.3521, 0.6479]],\n",
              "\n",
              "        [[0.5117, 0.4883],\n",
              "         [0.7114, 0.2886]],\n",
              "\n",
              "        [[0.1633, 0.8367],\n",
              "         [0.2770, 0.7230]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yq4dGW_Oadi",
        "colab_type": "code",
        "outputId": "c133cc47-7b68-46ef-b43e-8d5db4cc185f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Sample soft categorical using reparametrization trick:\n",
        "F.gumbel_softmax(logits, tau=1, hard=False)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.8818, 0.1182],\n",
              "         [0.6876, 0.3124]],\n",
              "\n",
              "        [[0.8448, 0.1552],\n",
              "         [0.0220, 0.9780]],\n",
              "\n",
              "        [[0.1111, 0.8889],\n",
              "         [0.3510, 0.6490]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c3k2l8UjOadk",
        "colab_type": "code",
        "outputId": "5672bce6-e443-4e82-9268-1080a45d2d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Sample hard categorical using \"Straight-through\" trick:\n",
        "F.gumbel_softmax(logits, tau=1, hard=True)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 0.],\n",
              "         [1., 0.]],\n",
              "\n",
              "        [[1., 0.],\n",
              "         [1., 0.]],\n",
              "\n",
              "        [[1., 0.],\n",
              "         [0., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVXSeUFzOadm",
        "colab_type": "code",
        "outputId": "f7c35157-0333-4874-c0dc-a2c2103e3a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Sample hard categorical using \"Straight-through\" trick:\n",
        "F.gumbel_softmax(logits, tau=1, hard=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 1.],\n",
              "         [1., 0.]],\n",
              "\n",
              "        [[1., 0.],\n",
              "         [0., 1.]],\n",
              "\n",
              "        [[1., 0.],\n",
              "         [0., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}
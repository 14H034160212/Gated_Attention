{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "D0T3nxESJstz",
    "outputId": "7341ce37-5134-467c-9d54-16650d84097e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 759kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/site-packages (from torchtext==0.4) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from torchtext==0.4) (1.18.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from torchtext==0.4) (2.22.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from torchtext==0.4) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from torchtext==0.4) (4.42.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->torchtext==0.4) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->torchtext==0.4) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->torchtext==0.4) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->torchtext==0.4) (2.8)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.4.0\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9sHZi8rIXBC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import torch.distributions\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "# from models.LSTM import LSTMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CtSKjSRaQdd"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "class GANet(torch.nn.Module):\n",
    "\t\tdef __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, aux_hidden_size = 100, backbone_hidden_size = 100, tau = 1, biDirectional_aux = False, biDirectional_backbone = False):\n",
    "\t\t\tsuper(GANet, self).__init__() \n",
    "\t\t\t\"\"\"\n",
    "\t\t\tArguments\n",
    "\t\t\t---------\n",
    "\t\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "\t\t\toutput_size : 6 = (For TREC dataset)\n",
    "\t\t\thidden_sie : Size of the hidden_state of the LSTM   (// Later BiLSTM)\n",
    "\t\t\tvocab_size : Size of the vocabulary containing unique words\n",
    "\t\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
    "\t\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "\t\t\t--------\n",
    "\n",
    "\t\t\t\"\"\"\n",
    "\n",
    "\t\t\tself.batch_size = batch_size\n",
    "\t\t\tself.num_classes = num_classes\n",
    "\t\t\tself.vocab_size = vocab_size\n",
    "\t\t\tself.embedding_length = embedding_length\n",
    "\t\t\tself.aux_hidden_size = aux_hidden_size\n",
    "\t\t\tself.backbone_hidden_size = backbone_hidden_size \n",
    "\t\t\tself.mlp_out_size = mlp_out_size\n",
    "\t\t\tself.biDirectional_aux = biDirectional_aux\n",
    "\t\t\tself.biDirectional_backbone = biDirectional_backbone\n",
    "\t\t\tself.tau = tau\n",
    "\n",
    "\t\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "\t\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "\n",
    "\t\t\tself.auxiliary = AuxiliaryNet(self.batch_size, self.aux_hidden_size, self.embedding_length, self.biDirectional_aux, tau = self.tau)\n",
    "\t\t\tself.backbone = BackboneNet(self.batch_size, self.backbone_hidden_size, self.embedding_length, self.biDirectional_backbone)\n",
    "\t \n",
    "\t\t\tif(self.biDirectional_backbone):\n",
    "\t\t\t\tself.mlp = MLP(self.backbone_hidden_size * 2, self.mlp_out_size)\n",
    "\t\t\t\tself.FF = nn.Linear(self.backbone_hidden_size * 2,num_classes)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.mlp = MLP(self.backbone_hidden_size, self.mlp_out_size)\n",
    "\t\t\t\tself.FF = nn.Linear(self.backbone_hidden_size,num_classes)\n",
    "\t\t\t# self.softmax = nn.Softmax(dim = -1)\n",
    "\t\t\t\n",
    "\n",
    "\t\tdef masked_Softmax(self, logits, mask):\n",
    "\t\t\tmask_bool = mask >0\n",
    "\t\t\tlogits[~mask_bool] = float('-inf')\n",
    "\t\t\treturn torch.softmax(logits, dim=1)\t\n",
    "\n",
    "\t\tdef forward(self,input_sequence, is_train = True):\n",
    "\t\t\tinput_ = self.word_embeddings(input_sequence)\n",
    "\t\t\tg_t = self.auxiliary(input_, is_train)\n",
    "\t\t\tout_lstm = self.backbone(input_)\n",
    "\n",
    "\t\t\tif is_train:\n",
    "\t\t\t\te_t = self.mlp(out_lstm)\n",
    "\t\t\t\talpha = torch.softmax(e_t, dim = 1)\n",
    "\t\t\telse:\n",
    "\t\t\t\te_t = self.mlp(out_lstm)               # change if possible!\n",
    "\t\t\t\talpha = self.masked_Softmax(e_t, g_t)\n",
    "\n",
    "\t\t\tc_t = torch.bmm(alpha.transpose(1,2), out_lstm)\n",
    "\t\t\tlogits = self.FF(c_t)\n",
    "\t\t\tfinal_output = torch.softmax(logits, dim = -1)\n",
    "\t\t\t# final_output = final_output.max(2)[1]\n",
    "\t\t\tfinal_output = final_output.squeeze(1)\n",
    "\n",
    "\n",
    "\t\t\treturn final_output, g_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pdl-jRmHb5KA"
   },
   "outputs": [],
   "source": [
    "class AuxiliaryNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "    aux_hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
    "    embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "    --------\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, auxiliary_hidden_size, embedding_length, biDirectional = False, num_layers = 1, tau=1):\n",
    "        super(AuxiliaryNet, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = auxiliary_hidden_size\n",
    "        self.embedding_length = embedding_length\t\n",
    "        self.biDirectional\t= biDirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.aux_lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, num_layers = self.num_layers, batch_first = True)   # Dropout  \n",
    "        if(self.biDirectional):\n",
    "            self.aux_linear = nn.Linear(self.hidden_size * 2,1)\n",
    "        else:\n",
    "            self.aux_linear = nn.Linear(self.hidden_size,1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.tau = tau\n",
    "\n",
    "\n",
    "    def forward(self, input_sequence, is_train = True, batch_size=None):\n",
    "\n",
    "        # input : Dimensions (batch_size x seq_len x embedding_length)\n",
    "        out_lstm, (final_hidden_state, final_cell_state) = self.aux_lstm(input_sequence) # ouput dim: (batch_size x seq_len x hidden_size) \n",
    "        out_linear = self.aux_linear(out_lstm)                                           # p_t dim: (batch_size x seq_len x 1)\n",
    "        p_t = self.sigmoid(out_linear)\n",
    "\n",
    "        if is_train:\n",
    "            p_t = p_t.repeat(1,1,2)\n",
    "            p_t[:,:,0] = 1 - p_t[:,:,0] \n",
    "            g_hat = F.gumbel_softmax(p_t, self.tau, hard=False)   \n",
    "            g_t = g_hat[:,:,1]\n",
    "        else:\n",
    "            # size : same as p_t [ batch_size x seq_len x 1]\n",
    "            m = torch.distributions.bernoulli.Bernoulli(p_t)   \n",
    "            g_t = m.sample()\n",
    "        return g_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT9xWP_MAVmq"
   },
   "outputs": [],
   "source": [
    "class BackboneNet(torch.nn.Module):\n",
    "\t\"\"\"\n",
    "\t\tArguments\n",
    "\t\t---------\n",
    "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "\t\tbackbone_hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
    "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
    "\t\t--------\n",
    "\t\t\"\"\"\n",
    "\tdef __init__(self, batch_size, backbone_hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
    "\n",
    "\t\tsuper(BackboneNet, self).__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.hidden_size = backbone_hidden_size\n",
    "\t\tself.embedding_length = embedding_length\n",
    "\t\tself.biDirectional\t= biDirectional\n",
    "\t\tself.num_layers = num_layers\n",
    "\n",
    "\t\tself.backbone_lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
    "\n",
    "\tdef forward(self, input_sequence, batch_size=None):\n",
    "\t\tout_lstm, (final_hidden_state, final_cell_state) = self.backbone_lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
    "\t\treturn out_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_eU0HR-j8lW"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff_2 = nn.Linear(self.output_dim,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_1 = self.ff_1(x)\n",
    "        out_relu = self.relu(out_1)\n",
    "        out_2 = self.ff_2(out_relu)\n",
    "        out_sigmoid = self.sigmoid(out_2)\n",
    "\n",
    "        return out_sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Swz_WT2mS3zq"
   },
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "    \n",
    "def train_model(model, train_iter, epoch, batch_size):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    # model.cuda()\n",
    "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not batch_size):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction, g_t = model(text, is_train = True)\n",
    "        # print(\"prediction = \", prediction.shape)\n",
    "        # print(\"target = \", target.shape)\n",
    "        # print(\"prediction = \", prediction)\n",
    "        # print(\"target = \", target)\n",
    "        # loss = loss_fn(prediction, target, g_t)\n",
    "        loss =  loss_fn(prediction, target)\n",
    "        # print(\"loss = \", loss)\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        # if steps % 10 == 0:\n",
    "            # print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "            # break\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    total_attention =  0\n",
    "    total_samples = 0 \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction, g_t = model(text, is_train = False)\n",
    "            # Sanity check\n",
    "            # print(\"Test Prediction: \", prediction)\n",
    "            # print(\"Gate values: \", g_t)\n",
    "\n",
    "            # For density calculation\n",
    "            total_attention += torch.sum(g_t)\n",
    "            # print(total_attention)\n",
    "            # print(g_t.shape)\n",
    "            total_samples += g_t.shape[0] * g_t.shape[1]\n",
    "            # loss = loss_fn(prediction, target, g_t)\n",
    "            loss =  loss_fn(prediction, target)\n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter), total_attention/total_samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAjlrmkGSEQQ"
   },
   "outputs": [],
   "source": [
    "# data.py\n",
    "def load_TREC_data(batch_size= 32, embedding_length = 100):\n",
    "    # set up fields\n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length= 10)\n",
    "    # LABEL = data.LabelField()\n",
    "    LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "    # make splits for data\n",
    "    train, test = datasets.TREC.splits(TEXT, LABEL)\n",
    "\n",
    "    # build the vocabulary\n",
    "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=embedding_length))\n",
    "    LABEL.build_vocab(train)\n",
    "    print(LABEL.vocab.__dict__)\n",
    "\n",
    "    # make iterator for splits\n",
    "    train_iter, test_iter = data.BucketIterator.splits(\n",
    "      (train, test), batch_size= batch_size, device=0)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "xHuvlEncW_tw",
    "outputId": "ea08c54e-f4c1-4785-cf86-c072a8dd620f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 1.74MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 427kB/s]\n",
      ".vector_cache/glove.6B.zip: 862MB [32:52, 437kB/s]                                \n",
      " 99%|█████████▉| 397670/400000 [00:16<00:00, 23881.79it/s]WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freqs': Counter({'ENTY': 1250, 'HUM': 1223, 'DESC': 1162, 'NUM': 896, 'LOC': 835, 'ABBR': 86}), 'itos': ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR'], 'unk_index': None, 'stoi': defaultdict(None, {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5}), 'vectors': None}\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "TEXT, vocab_size, word_embeddings, train_iter, test_iter = load_TREC_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkoJVXJ2mzCD"
   },
   "outputs": [],
   "source": [
    "def loss_fn(output, target, g_t, lambda_ = 1e-4):\n",
    "  T = len(g_t)\n",
    "  # loss = -nn.LogSoftmax(output[target], dim = 1) + (lambda_ * torch.sum(g_t))/T\n",
    "  loss = F.cross_entropy(output, target) + (lambda_ * torch.sum(g_t))/T\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PImJSOJmzQgT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 397670/400000 [00:30<00:00, 23881.79it/s]"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "output_size = 2\n",
    "hidden_size = 256\n",
    "embedding_length = 100\n",
    "num_classes = 6\n",
    "mlp_out_size = 32\n",
    "weights = word_embeddings\n",
    "aux_hidden_size = 100\n",
    "batch_hidden_size = 100\n",
    "tau = 0.5\n",
    "\n",
    "model = GANet(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, tau= tau, biDirectional_aux=False, biDirectional_backbone=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "colab_type": "code",
    "id": "EcU6SSW8bDln",
    "outputId": "49069342-b9dd-4d4c-d2f3-6509dd836057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 1.062, Train Acc: 96.97%\n",
      "Epoch: 02, Train Loss: 1.062, Train Acc: 96.97%\n",
      "Epoch: 03, Train Loss: 1.061, Train Acc: 97.09%\n",
      "Epoch: 04, Train Loss: 1.061, Train Acc: 97.04%\n",
      "Epoch: 05, Train Loss: 1.061, Train Acc: 97.06%\n",
      "Epoch: 06, Train Loss: 1.061, Train Acc: 97.06%\n",
      "Epoch: 07, Train Loss: 1.060, Train Acc: 97.15%\n",
      "Epoch: 08, Train Loss: 1.061, Train Acc: 97.04%\n",
      "Epoch: 09, Train Loss: 1.061, Train Acc: 97.09%\n",
      "Epoch: 10, Train Loss: 1.060, Train Acc: 97.13%\n",
      "Epoch: 11, Train Loss: 1.060, Train Acc: 97.13%\n",
      "Epoch: 12, Train Loss: 1.061, Train Acc: 97.09%\n",
      "Epoch: 13, Train Loss: 1.060, Train Acc: 97.15%\n",
      "Epoch: 14, Train Loss: 1.060, Train Acc: 97.19%\n",
      "Epoch: 15, Train Loss: 1.059, Train Acc: 97.22%\n",
      "Epoch: 16, Train Loss: 1.059, Train Acc: 97.22%\n",
      "Epoch: 17, Train Loss: 1.060, Train Acc: 97.13%\n",
      "Epoch: 18, Train Loss: 1.061, Train Acc: 97.08%\n",
      "Epoch: 19, Train Loss: 1.059, Train Acc: 97.22%\n",
      "Epoch: 20, Train Loss: 1.059, Train Acc: 97.24%\n",
      "Epoch: 21, Train Loss: 1.059, Train Acc: 97.31%\n",
      "Epoch: 22, Train Loss: 1.058, Train Acc: 97.33%\n",
      "Epoch: 23, Train Loss: 1.059, Train Acc: 97.30%\n",
      "Epoch: 24, Train Loss: 1.058, Train Acc: 97.33%\n",
      "Epoch: 25, Train Loss: 1.058, Train Acc: 97.33%\n",
      "Epoch: 26, Train Loss: 1.059, Train Acc: 97.30%\n",
      "Epoch: 27, Train Loss: 1.058, Train Acc: 97.31%\n",
      "Epoch: 28, Train Loss: 1.059, Train Acc: 97.26%\n",
      "Epoch: 29, Train Loss: 1.058, Train Acc: 97.33%\n",
      "Epoch: 30, Train Loss: 1.058, Train Acc: 97.37%\n",
      "Epoch: 31, Train Loss: 1.058, Train Acc: 97.35%\n",
      "Epoch: 32, Train Loss: 1.058, Train Acc: 97.39%\n",
      "Epoch: 33, Train Loss: 1.058, Train Acc: 97.39%\n",
      "Epoch: 34, Train Loss: 1.057, Train Acc: 97.40%\n",
      "Epoch: 35, Train Loss: 1.057, Train Acc: 97.44%\n",
      "Epoch: 36, Train Loss: 1.057, Train Acc: 97.46%\n",
      "Epoch: 37, Train Loss: 1.057, Train Acc: 97.44%\n",
      "Epoch: 38, Train Loss: 1.057, Train Acc: 97.50%\n",
      "Epoch: 39, Train Loss: 1.057, Train Acc: 97.50%\n",
      "Epoch: 40, Train Loss: 1.056, Train Acc: 97.50%\n",
      "Epoch: 41, Train Loss: 1.056, Train Acc: 97.57%\n",
      "Epoch: 42, Train Loss: 1.056, Train Acc: 97.53%\n",
      "Epoch: 43, Train Loss: 1.055, Train Acc: 97.62%\n",
      "Epoch: 44, Train Loss: 1.055, Train Acc: 97.62%\n",
      "Epoch: 45, Train Loss: 1.055, Train Acc: 97.62%\n",
      "Epoch: 46, Train Loss: 1.056, Train Acc: 97.61%\n",
      "Epoch: 47, Train Loss: 1.055, Train Acc: 97.62%\n",
      "Epoch: 48, Train Loss: 1.055, Train Acc: 97.62%\n",
      "Epoch: 49, Train Loss: 1.056, Train Acc: 97.55%\n",
      "Epoch: 50, Train Loss: 1.055, Train Acc: 97.61%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train_loss, train_acc = train_model(model, train_iter, epoch, batch_size)\n",
    "    # val_loss, val_acc = eval_model(model, valid_iter)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%')\n",
    "    # print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "vwMLkn7s7cnC",
    "outputId": "86182669-19b9-4b8d-b9a4-815acf8d240e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.136, Test Acc: 77.54, Density: 0.4812 \n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, density = eval_model(model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}, Density: {density:.4f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kj2OOxjKaAIm"
   },
   "outputs": [],
   "source": [
    "def test_sentence(test_sen):\n",
    "\n",
    "  test_sen = TEXT.preprocess(test_sen)\n",
    "  print(test_sen)\n",
    "  test_sen = [[TEXT.vocab.stoi[x] for x in test_sen]]\n",
    "  # print(test_sen)\n",
    "\n",
    "  test_sen = np.asarray(test_sen)\n",
    "  test_sen = torch.LongTensor(test_sen)\n",
    "  test_tensor = Variable(test_sen, volatile=True)\n",
    "\n",
    "  # print(test_tensor)\n",
    "  model.eval()\n",
    "  prediction, g_t = model(test_tensor, is_train = False)\n",
    "  print(\"prediction =\", prediction)\n",
    "  print(\"g =\", g_t)\n",
    "  out_class = torch.argmax(prediction)\n",
    "  return out_class\n",
    "\n",
    "# ['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR']\n",
    "test_sen0 = \"What does the six-footed Musca domestica become when it enters a house ?\" # class = Entity - 0\n",
    "test_sen1 = \"Who killed Gandhi?\"   # Class: HUM - 1\n",
    "test_sen2 = \"What does target heart rate mean ?\" # class = \"DESC\"\n",
    "test_sen3 = \"How old was Joan of Arc when she died ?\" # class = \"NUM\"\n",
    "test_sen4 = \"Where on the body is a mortarboard worn ?\" # class = \"LOC\"\n",
    "test_sen5 = \"What does I.V. stand for ?\" # class = \"ABBR\"\n",
    "x = test_sentence(test_sen5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJf5OMBaO8Wl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GANet_Torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
